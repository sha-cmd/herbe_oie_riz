{
  
    
        "post0": {
            "title": "Math en Latex",
            "content": "Greek letters . alpha $ alpha$ A nu N $ nu$ $N$ $ beta$ $B$ xi Xi $ xi$ $ Xi$ . gamma Gamma $ gamma$ $ Gamma$ o O $o$ $O$ delta Delta } delta Delta pi Pi } pi Pi epsilon varepsilon E ;} epsilon varepsilon E rho varrho P ;} rho varrho P zeta Z} zeta Z sigma ,! Sigma ;} sigma Sigma eta H} eta H tau T} tau T theta vartheta Theta } theta vartheta Theta upsilon Upsilon } upsilon Upsilon iota I} iota I phi varphi Phi } phi varphi Phi kappa K} kappa K chi X} chi X lambda Lambda ;} lambda Lambda psi Psi } psi Psi mu M} mu M omega Omega } omega Omega Arrows leftarrow } leftarrow Leftarrow } Leftarrow rightarrow } rightarrow Rightarrow ;} Rightarrow leftrightarrow } leftrightarrow rightleftharpoons } rightleftharpoons uparrow } uparrow downarrow } downarrow Uparrow ;} Uparrow Downarrow } Downarrow Leftrightarrow ;} Leftrightarrow Updownarrow } Updownarrow mapsto } mapsto longmapsto ;} longmapsto nearrow } nearrow searrow } searrow swarrow } swarrow nwarrow } nwarrow leftharpoonup } leftharpoonup rightharpoonup } rightharpoonup leftharpoondown } leftharpoondown rightharpoondown } rightharpoondown Miscellaneous symbols infty ; ;} infty forall ;} forall Re } Re Im } Im nabla } nabla exists } exists partial } partial nexists } nexists emptyset } emptyset varnothing ;} varnothing wp } wp complement } complement neg } neg cdots } cdots square } square surd } surd blacksquare } blacksquare triangle } triangle Binary Operation/Relation Symbols times } times times } times div } div cap } cap cup } cup neq ;} neq leq } leq geq } geq in } in perp ;} perp notin } notin subset } subset simeq } simeq approx } approx wedge } wedge vee } vee oplus ;} oplus otimes } otimes Box } Box boxtimes } boxtimes . equiv } equiv cong } cong .",
            "url": "https://blog.rboyrie.info/statistiques/2021/03/05/Latex_Math.html",
            "relUrl": "/statistiques/2021/03/05/Latex_Math.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistiques Bayesiennes",
            "content": "Th&#233;or&#232;me de Bayes : . Voici tout se dont vous devez savoir à propos des statistiques (R. Feynman) : . $P( theta|y) = frac{P(y| theta) cdot P( theta)}{P(y)}$ . D’après la règle des produits : . $P( theta,y) = P( theta|y)P(y)$ . ou . $P( theta,y) = P(y| theta)P( theta)$ . Ce qui équivaut en les combinant à : . $P( theta|y)P(y) = P(y| theta)P( theta)$ . Si nous remplaçons $ theta$ par l’hypothèse et y par les data, le théorème de Bayes nous dit comment calculer la probabilité de notre hypothèse sachant nos données. L’hypothèse est transformé en probabilité en utilisant des distributions de probabilités. . $P( theta)$ : Primauté, les statisticiens la qualifie de subjective. . $P(y| theta)$ : La certitude, le modèle de l’échantillon, le modèle statisque ou le modèle. . $P( theta|y)$ : Le posterieur de la distribution, c’est une distribution de probabilité pour notre paramètre $ theta$ et non une simple valeur. . $P(y)$ : Probabilité marginale, ou évidence. Peut-être ignoré par les débutants, en ne la considérant que comme une valeur relative et non pas comme une valeur absolue. . &#192; quoi sert-il . Les probabilités sont des outils pour mesurer l’incertitude à propos de paramètres, et le théorème de Bayes est le méchanisme pour mettre à jour correctement ces probabilités à la lumière de nouvelles données, avec l’espoir de pouvoir réduire l’incertitude. .",
            "url": "https://blog.rboyrie.info/statistiques/2021/03/05/Bayesian-Statistics.html",
            "relUrl": "/statistiques/2021/03/05/Bayesian-Statistics.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Statistiques",
            "content": "Ind&#233;pendance . Deux événements sont indépendants si ce qui suit est vrai : | . $P(A|B) = P(A) cdot P(B|A) = P(B) cdot P(A ET B)$ . $P(A ET B) = P(A) cdot P(B)$ . Quand P(B) ne vaut ni 0, ni 1, les deux événements A et B sont indépendant si : . $P(A) = P(A|B)$ . Deux événements sont indépendants si la connaissance que l’un s’est produit n’affecte pas la chance que l’autre se produise. | . Prouver l&#8217;ind&#233;pendance de deux variables (1 condition parmi les 3 suivantes suffit) : . Si deux événements ne sont PAS indépendants, alors nous disons qu’ils sont indépendants. | . L’échantillonage peut être fait avec replacement ou sans replacement : . Avec replacement : Lorsque l’échantillonage est réalisé avec replacement, les événements sont considérés comme indépendants, ce qui signifie que les résultats du premier ne change pas le résultat du deuxième prélèvement. . | Sans replacement : Lorsque l’échantillonage est réalisé sans replacement, chaque membre d’une population peut être choisi uniquement une fois. Dans ce cas les probabilités pour le deuxième choix sont affectées par le résultat du premier choix. Les événements sont considérés comme dépendants ou non indépendants. . | Distribution Binomiale . Il existe trois caractéristiques de la distribution binomiale. . 1-. Il existe un nombre fixe d’essais. La lettre $n$ désigne le nombre d’essais. C’est le cas dans la répétition d’une expérience. . Il n’y a que deux résultats possibles, «succés» ou «échecs», pour chaque essais. La lettre $p$ indique la probabilité de succés sur un essai, et $q$ indique la probabilité d’échec sur un essai. $p + q = 1$ . | Les essais $n$ sont indépendants et sont répétés dans des conditions identiques. Comme les essais sont indépendants, les probabilités $p$ et $q$ sont les mêmes pour chaque essais. . | La loi de comportement est $f(k) = binom{n}{k}p^k(1-p)^{n-k}$ . from scipy.stats import binom . n, p, k, loc = 5, 0.4, 3, 0# k est l’essais qui nous intéresse pour la mesure binom.pmf(k - loc, n, p) . 0.2304 . Distribution G&#233;om&#233;trique . Il existe trois caractéristiques principales d’une expérience géométrique. . Il y a un ou plusieurs essais de Bernoulli avec tous les échecs sauf le dernier, qui est un succés. En d’autres termes, vous répétez ce que vous faites jusqu’au premier succés. Ensuite, vous arrêtez. . | En théorie le nombre d’essais peut durer indéfiniment. Il doit y avoir au moins un essai. . | La probabilité $p$ d’un succés et la probabilité $q$ d’un échec sont les mêmes pour chaque essai. $p + q = 1$ et $q = 1 - p$. La probabilité d’obtenir un 3 en lançant un dé est toujours de $ frac{1}{6}$. . | La probabilité d’obtenir un 3 au cinquième lancé est : $( frac{5}{6}) cdot( frac{5}{6}) cdot( frac{5}{6}) cdot( frac{5}{6}) cdot( frac{1}{6}) = 0.080375514403293$ . La loi de comportement est $f(k) = (1 - p)^{(k-1)}p$ . from scipy.stats import geom . p, k, loc = 1/6, 5, 0 # loc sert à décalé la position de départ du décompte de la distribution. geom.pmf(k - loc, p) . 0.08037551440329219 . Distribution Hyperg&#233;om&#233;trique . Il existe cinq caractéristiques d’une expérience hypergéométrique . Vous prélevez des échantillons de deux groupes. . | Vous êtes concerné par un groupe d’intérêt, appelé le premier groupe. . | Vous échantilloner sans remplacement à partir des groupes combinés. Par exemple, vous voulez choisir une équipe de softball parmi un groupe combiné de 11 hommes et 13 femmes. L&#39;équipe se compose de dix joueurs. . | Chaque prélèvement n&#39;est pas indépendant, car l&#39;échantillonnage est sans remplacement. Dans l&#39;exemple du softball, la probabilité de choisir une femme en premier est de $ frac{13}{24}$. La probabilité de choisir un homme en second est $ frac{11}{23}$ si une femme a été choisie en premier. C&#39;est $ frac{10}{23}$ si un homme a été choisi en premier. La probabilité du deuxième choix dépend de ce qui s&#39;est passé lors du premier choix. . | Vous ne traitez pas avec les essais de Bernoulli. Les résultats d&#39;une expérience hypergéométrique correspondent à une distribution de probabilité hypergéométrique. La variable aléatoire $X = $le nombre d&#39;items du groupe d&#39;intérêt. . | La probabilité de masse est : $p(k, M, n, N) = frac{ binom{n}{k} binom{M - n}{N - k}}{ binom{M}{N}}$ . le coefficient binomial est : $ binom{n}{k}≡ frac{n!}{k!(n - k)!}$ . from scipy.stats import hypergeom . k, M, n, N = 3, 20, 7, 12 # k nombre d’éléments du groupe d’intérêt tirés, M nombre d’éléments combinés, # n nombre d’éléments du premier groupe, N nombre d’éléments tirés hypergeom.pmf(k - loc, M, n, N) . 0.19865841073271373 . Distribution de Poisson . Il existe deux caractéristiques principales d’une expérience de Poisson. . La distribution de probabilité de Poisson donne la probabilité qu&#39;un certain nombre d&#39;événements se produisent dans un intervalle de temps ou d&#39;espace fixe si ces événements se produisent avec un taux moyen connu et indépendamment du temps écoulé depuis le dernier événement. Par exemple, un éditeur de livre pourrait être intéressé par le nombre de mots mal orthographiés dans un livre particulier. Il se peut qu&#39;en moyenne, il y ait cinq mots mal orthographiés sur 100 pages. L&#39;intervalle correspond aux 100 pages. . | La distribution de Poisson peut être utilisée pour approcher le binôme si la probabilité de succès est &quot;petite&quot; (comme 0,01) et le nombre d&#39;essais est &quot;grand&quot; (comme 1 000). $n$ est le nombre d&#39;essais et $p$ la probabilité d&#39;un &quot;succès&quot;. La variable aléatoire $X =$ le nombre d&#39;occurrences dans l&#39;intervalle d&#39;intérêt. . | La probabilité de masse est : $f(k) = exp(- mu) frac{ mu^{k}}{k!}$ pour $k ge0$ . from scipy.stats import poisson . mu, k = 0.05, 100 poisson.cdf(k - loc, mu) . 1.0 . La distribution uniforme . La distribution uniforme est une distribution de probabilité continue et concerne les événements qui sont également susceptibles de se produire. Lorsque vous travaillez sur des problèmes qui ont une distribution uniforme, veillez à noter si les données incluent ou excluent les points de terminaison. . from scipy.stats import uniform . x, scale, loc =9, 24, 0 uniform.pdf(x, loc, scale) . 0.041666666666666664 . La distribution exponentielle . La distribution exponentielle concerne souvent le temps écoulé jusqu&#39;à ce qu&#39;un événement spécifique se produise. . Par exemple, le temps (à partir de maintenant) jusqu&#39;à ce qu&#39;un tremblement de terre se produise a une distribution exponentielle. . D&#39;autres exemples incluent la durée, en minutes, des appels téléphoniques longue distance d&#39;affaires et la durée, en mois, d&#39;une batterie de voiture. . Il peut également être montré que la valeur du changement que vous avez dans votre poche ou votre sac à main suit approximativement une distribution exponentielle. . Les valeurs d&#39;une variable aléatoire exponentielle se produisent de la manière suivante. Il y a moins de grandes valeurs et plus de petites valeurs. . Par exemple, le montant d&#39;argent que les clients dépensent en un voyage au supermarché suit une distribution exponentielle. . Il y a plus de gens qui dépensent de petites sommes d&#39;argent et moins de gens qui dépensent de grandes sommes d&#39;argent. Les distributions exponentielles sont couramment utilisées dans les calculs de la fiabilité des produits ou de la durée de vie d&#39;un produit. . from scipy.stats import expon . x, scale = 100, 200 expon.pdf(x, loc=0, scale=1)# equivaut au paramètre lambda, tout comme en loi de Poisson . 3.720075976020836e-44 . La limite centrale . Si vous voulez comprendre la distribution du changement que les gens portent dans leurs poches, en utilisant le théorème de la limite centrale et en supposant que votre échantillon est suffisamment grand, vous constaterez que la distribution est normale et en forme de cloche. . Il existe deux formes alternatives du théorème, et les deux alternatives consistent à tirer des échantillons finis de taille $n$ à partir d&#39;une population avec une moyenne connue, $ mu$, et un écart type connu, $ sigma$. . La première alternative dit que si nous collectons des échantillons de taille $n$ avec un «assez grand $n$», calculons la moyenne de chaque échantillon et créons un histogramme de ces moyennes, alors l&#39;histogramme résultant aura tendance à avoir une forme de cloche normale approximative. . | La deuxième alternative dit que si nous collectons à nouveau des échantillons de taille $n$ qui sont &quot;assez grands&quot;, calculons la somme de chaque échantillon et créons un histogramme, alors l&#39;histogramme résultant aura à nouveau tendance à avoir une forme de cloche normale. . | L&#39;échantillon, $n$, qui est en d’autres termes «suffisamment grand», dépend de la population d&#39;origine à partir de laquelle les échantillons sont tirés (la taille de l&#39;échantillon doit être d&#39;au moins 30 ou les données doivent provenir d&#39;une distribution normale. . | Si la population d&#39;origine est loin d&#39;être normale, il faudra alors davantage d&#39;observations pour que les moyennes ou les sommes de l&#39;échantillon soient normales. L&#39;échantillonnage se fait avec remplacement. . | import numpy as np import matplotlib.pyplot as plt . Code issue de la page github de Rajesh Singh. . f = plt.figure(figsize=(18, 10)) def plotHist(nr, N, n_, mean, var0, x0): &#39;&#39;&#39; plots the RVs&#39;&#39;&#39; x = np.zeros((N)) sp = f.add_subplot(3, 2, n_ ) for i in range(N): for j in range(nr): x[i] += np.random.random() x[i] *= 1/nr plt.hist(x, 100, density=True, color=&#39;#348ABD&#39;, label=&quot; %d RVs&quot;%(nr)); plt.setp(sp.get_yticklabels(), visible=False) variance = var0/nr fac = 1/np.sqrt(2*np.pi*variance) dist = fac*np.exp(-(x0-mean)**2/(2*variance)) plt.plot(x0,dist,color=&#39;#A60628&#39;,linewidth=3,label=&#39;CLT&#39;,alpha=0.8) plt.xlabel(&#39;r&#39;) plt.xlim([0, 1]) leg = plt.legend(loc=&quot;upper left&quot;) leg.get_frame().set_alpha(0.1) N = 10000 # number of samples taken nr = ([1, 2, 4, 8, 16, 32]) mean, var0 = 0.5, 1.0/12 # mean and variance of uniform distribution in range 0, 1 x0 = np.linspace(0, 1, 128) for i in range(np.size(nr)): plotHist(nr[i], N, i+1, mean, var0, x0) plt.suptitle(&quot;Addition of uniform random variables (RVs) converge to a Gaussian distribution (CLT)&quot;,fontsize=20); . Le th&#233;or&#232;me central des limites pour les moyennes d&#39;&#233;chantillonnage . Supposons que X soit une variable aléatoire avec une distribution qui peut être connue ou inconnue (il peut s&#39;agir de n&#39;importe quelle distribution). En utilisant un indice qui correspond à la variable aléatoire, supposons: . a. $ mu_X =$ la moyenne de X . b. $ sigma_X =$ l&#39;écart type de X . Si vous tirez des échantillons aléatoires de taille, alors à mesure que $n$ augmente, la variable aléatoire $ bar{X}$ qui consiste en des moyennes d&#39;échantillon, tend à être normalement distribuée $ bar{X}_{ mapsto infty} approx N( mu_X, sigma_X)$ . Le théorème de limite central pour les moyennes d&#39;échantillon indique que si vous continuez à dessiner des échantillons de plus en plus grands (comme lancer un, deux, cinq et enfin dix dés) et à calculer leurs moyennes, les moyennes d&#39;échantillon forment leur propre distribution normale (la distribution d&#39;échantillonnage). . La distribution normale a la même moyenne que la distribution d&#39;origine et une variance égale à la variance d&#39;origine divisée par la taille de l&#39;échantillon. L&#39;écart type est la racine carrée de la variance, de sorte que l&#39;écart type de la distribution d&#39;échantillonnage est l&#39;écart type de la distribution d&#39;origine divisé par la racine carrée de $n$. . La variable $n$ est le nombre de valeurs moyennées ensemble, et non le nombre de fois où l&#39;expérience est effectuée. Pour le dire plus formellement, si vous tirez des échantillons aléatoires de taille acceptable, la distribution de la variable aléatoire $ bar{X}$, qui consiste en des moyennes d&#39;échantillon, est appelée distribution d&#39;échantillonnage de la moyenne. . La distribution d&#39;échantillonnage de la moyenne se rapproche de la distribution normale lorsque $n$, la taille de l&#39;échantillon, augmente. La variable aléatoire $ bar{X}$ a un score $z$ différent de celui de la variable aléatoire $X$. La moyenne $ bar{x}$ est la valeur de $ bar{X}$ dans un échantillon. $z = frac{ bar{x} − mu_x}{ frac{ sigma_x}{( sqrt[]{n})}}$ . $ mu_X$ est la moyenne de $X$ et de $ bar{X}$. . $ sigma_{ bar{x}} = frac{ sigma_x}{ sqrt[]{n}} = $ écart type de $ bar{X}$ et est appelée l’erreur srandard de la moyenne. . import numpy as np from scipy.stats import sem # Standard error of the means . a = np.arange(20).reshape(5,4) sem(a) . array([2.82842712, 2.82842712, 2.82842712, 2.82842712]) . sem(a, axis=None, ddof=0) . 1.2893796958227628 . Le th&#233;or&#232;me central de la limite des sommes . Supposons que X est une variable aléatoire avec une distribution qui peut être connue ou inconnue (cela peut être n&#39;importe quelle distribution) et supposons : . a. $ mu_X = $ la moyenne de $Χ$ . b. $ sigma_X = $ l&#39;écart type de X . Si vous tirez des échantillons aléatoires de taille acceptable, alors au fur et à mesure que vous l’augmenterez, la variable aléatoire $ Sigma{X}$ constituée de sommes a tendance à être normalement distribuée et $ Sigma{X} approx $$ N left( (n) ( mu_Χ), (n) ( sigma_Χ) right) $. . Le théorème central limite des sommes dit que si vous continuez à prélever des échantillons de plus en plus grands et à prendre leurs sommes, les sommes forment leur propre distribution normale (la distribution d&#39;échantillonnage), qui se rapproche d&#39;une distribution normale à mesure que la taille de l&#39;échantillon augmente. . La distribution normale a une moyenne égale à la moyenne originale multipliée par la taille de l&#39;échantillon et un écart type égal à l&#39;écart type d&#39;origine multiplié par la racine carrée de la taille de l&#39;échantillon. . La variable aléatoire $ Sigma{X}$ a le score-$z$ suivant associé à : . a. $ Sigma{x}$ est une somme . b. $z = frac{ Sigma{X} - (n)( mu_X)}{( sqrt[]{n})( sigma_x)}$ . i. $(n)( mu_X) = $ the mean of $ Sigma{X}$ . ii. $(n)( sigma_X)= $ standard deviation of $ Sigma{X}$ . Hypoth&#232;ses nulles et alternatives . Le test proprement dit commence par considérer deux hypothèses. On les appelle l&#39;hypothèse nulle et l&#39;hypothèse alternative. . Ces hypothèses contiennent des points de vue opposés. . H0, L&#39;hypothèse nulle : Il s&#39;agit d&#39;un énoncé de l&#39;absence de différence entre les moyennes ou proportions de l&#39;échantillon ou d&#39;absence de différence entre la moyenne ou la proportion d&#39;un échantillon et une moyenne ou une proportion de population. En d&#39;autres termes, la différence est égale à 0. . Ha, L&#39;hypothèse alternative : C&#39;est une affirmation sur la population qui est contradictoire avec H0 et ce que nous concluons lorsque nous rejetons H0. Puisque les hypothèses nulle et alternative sont contradictoires, vous devez examiner les preuves pour décider si vous avez suffisamment de preuves pour rejeter l&#39;hypothèse nulle ou non. . Les preuves se présentent sous la forme d&#39;échantillons de données. Une fois que vous avez déterminé l&#39;hypothèse prise en charge par l&#39;échantillon, vous prenez une décision. . Il existe deux options pour prendre une décision. Elles sont «rejeter H0» si les informations de l&#39;échantillon favorisent l&#39;hypothèse alternative ou «ne pas rejeter H0» ou «refuser de rejeter H0» si les informations de l&#39;échantillon sont insuffisantes pour rejeter l&#39;hypothèse nulle. Symboles mathématiques utilisés dans H0 et Ha : . H0 Ha . égal (=) | différent de (≠) ou supérieur à (&gt;) ou inférieur à (&lt;) | | . supérieur ou égal à (≥) | inférieur à (&lt;) | | . inférieur ou égal à (≤) | supérieur à (&gt;) | | . Sortie et erreurs de type I et II . Action Ho est vraiment Ho est vraiment . | Vrai | Fausse | | . Ne pas rejeter Ho | Sortie Correcte | Erreur Type II $ beta$ | | . Rejeter Ho | Erreur Type I $ alpha$ | Sortie Correcte | . $ alpha = $ probabilité de Type I error = P(Type I error)= probabilité de rejeter l’hypothèse nulle quand elle est vrai. . $ beta = $ probabilité de Type II error =P(Type II error)= probabilité de ne pas rejeter l’hypothèse nulle quand elle est fausse. . Deux moyennes de population avec des &#233;carts types inconnus . Les deux échantillons indépendants sont de simples échantillons aléatoires provenant de deux populations distinctes. . | Pour les deux populations distinctes : . | si la taille des échantillons est petite, les distributions sont importantes (devraient être normales) | si la taille des échantillons est grande, les distributions ne sont pas importantes (il n&#39;est pas nécessaire qu&#39;elles soient normales) | . Le test comparant deux moyennes de population indépendantes avec des écarts-types de population inconnus et éventuellement inégaux s&#39;appelle le test t d&#39;Aspin-Welch. La formule des degrés de liberté a été développée par Aspin-Welch. . La comparaison de deux moyennes de population est très courante. Une différence entre les deux échantillons dépend à la fois des moyennes et des écarts types. Des moyens très différents peuvent se produire par hasard s&#39;il existe une grande variation entre les échantillons individuels. Afin de tenir compte de la variation, nous prenons la différence des moyennes de l&#39;échantillon, $ bar{X_1} – bar{X_2}$, et divisons par l&#39;erreur standard afin de normaliser la différence. . Le résultat est une statistique de test t-score. Comme nous ne connaissons pas les écarts-types de la population, nous les estimons en utilisant les deux écarts-types de l&#39;échantillon de nos échantillons indépendants. Pour le test d&#39;hypothèse, nous calculons l&#39;écart type estimé, ou l&#39;erreur standard, de la différence des moyennes d&#39;échantillonnage, $ bar{X_1} – bar{X_2}$. . Pour des variables dépendantes : . from scipy.stats import norm from scipy.stats import ttest_rel . np.random.seed(12345678) # fix random seed to get same numbers rvs1 = norm.rvs(loc=5,scale=10,size=500) rvs2 = (norm.rvs(loc=5,scale=10,size=500) + norm.rvs(scale=0.2,size=500)) ttest_rel(rvs1,rvs2) . Ttest_relResult(statistic=0.24101764965300979, pvalue=0.8096404344581155) . rvs3 = (norm.rvs(loc=8,scale=10,size=500) + norm.rvs(scale=0.2,size=500)) ttest_rel(rvs1,rvs3) . Ttest_relResult(statistic=-3.9995108708727924, pvalue=7.308240219166128e-05) . Pour des variables indépendantes : . from scipy.stats import norm from scipy.stats import ttest_ind . rvs1 = norm.rvs(loc=5,scale=10,size=500) rvs2 = norm.rvs(loc=5,scale=10,size=500) ttest_ind(rvs1,rvs2) . Ttest_indResult(statistic=1.0763821159276765, pvalue=0.282016525733433) . ttest_ind(rvs1,rvs2, equal_var = False) . Ttest_indResult(statistic=1.0763821159276765, pvalue=0.2820166316269163) . ttest_ind underestimates p for unequal variances: . rvs3 = norm.rvs(loc=5, scale=20, size=500) ttest_ind(rvs1, rvs3) . Ttest_indResult(statistic=0.8102978439129243, pvalue=0.41796218423074705) . ttest_ind(rvs1, rvs3, equal_var = False) . Ttest_indResult(statistic=0.8102978439129243, pvalue=0.4180327843951014) . When n1 != n2, the equal variance t-statistic is no longer equal to the unequal variance t-statistic: . rvs4 = norm.rvs(loc=5, scale=20, size=100) ttest_ind(rvs1, rvs4) . Ttest_indResult(statistic=-0.19824285974122152, pvalue=0.8429224292670197) . ttest_ind(rvs1, rvs4, equal_var = False) . Ttest_indResult(statistic=-0.1321360117182417, pvalue=0.8951183654547593) . T-test with different means, variance, and n: . rvs5 = norm.rvs(loc=8, scale=20, size=100) ttest_ind(rvs1, rvs5) . Ttest_indResult(statistic=-0.7536436880813698, pvalue=0.45135982629995197) . ttest_ind(rvs1, rvs5, equal_var = False) . Ttest_indResult(statistic=-0.49564652531472553, pvalue=0.6211387007521327) . LA DISTRIBUTION CHI-SQUARE . La distribution du chi carré peut être utilisée pour trouver des relations entre deux choses, comme les prix d&#39;épicerie dans différents magasins. . Dans ce type de test d&#39;hypothèse, vous déterminez si les données «correspondent» ou non à une distribution particulière. Par exemple, vous pouvez soupçonner que vos données inconnues correspondent à une distribution binomiale. Vous utilisez un test du chi carré (ce qui signifie que la distribution de l&#39;hypothèse est le chi carré) pour déterminer s&#39;il existe un ajustement ou non. . L&#39;hypothèse nulle et les hypothèses alternatives pour ce test peuvent être écrites dans des phrases ou peuvent être énoncées sous forme d&#39;équations ou d&#39;inégalités. La statistique de test pour un test de qualité d&#39;ajustement est : $ Sigma_{k} frac{(O − E)^2}{E}$ . où: . O = valeurs observées (données) | E = valeurs attendues (de la théorie) | k = le nombre de cellules de données différentes ou catégories | . La probabilité de masse est : $f(x, k) = frac{1}{2^{ frac{k}{2}} Gamma( frac{k}{2})}x^{ frac{k}{2-1}}exp( frac{-x}{2})$ . pour $x gt{0}$ et $k gt{0}$ (degrés de liberté, noté df dans l’implémentation . from scipy.stats import chi2 . x, df, var, scale = 1, 55, 0, 40 chi2.cdf(x, df, loc, scale) . 7.96460240704433e-82 . Effectuer et interpr&#233;ter le test du chi carr&#233; des tests d&#39;hypoth&#232;se d&#39;ind&#233;pendance. . Les tests d&#39;indépendance impliquent l&#39;utilisation d&#39;une table de contingence des valeurs (données) observées. La statistique de test pour un test d&#39;indépendance est similaire à celle d&#39;un test de qualité d&#39;ajustement : $ Sigma_{(i cdot j)} frac{(O – E)^2}{E}$ . où: . O = valeurs observées | E = valeurs attendues | i = le nombre de lignes dans le tableau | j = le nombre de colonnes du tableau | . Il existe $i cdot j$ termes de la forme $ frac{(O – E)^2}{E}$. . Un test d&#39;indépendance détermine si deux facteurs sont indépendants ou non. . Statistique de test : Utiliser une statistique de test $ chi^2$. . Il est calculé de la même manière que le test d&#39;indépendance. Degrés de liberté (df) $df = $ nombre de colonnes $- 1$ . Conditions requises : Toutes les valeurs du tableau doivent être supérieures ou égales à cinq. . Utilisations courantes : Comparaison de deux populations. . Par exemple: hommes vs femmes, avant vs après, est vs ouest. La variable est catégorique avec plus de deux valeurs de réponse possibles. . Effectuer et interpr&#233;ter des tests d&#39;hypoth&#232;se d&#39;homog&#233;n&#233;it&#233; du chi carr&#233;. . Le test de qualité d&#39;ajustement peut être utilisé pour décider si une population correspond à une distribution donnée, mais il ne suffira pas de décider si deux populations suivent la même distribution inconnue. Un test différent, appelé test d&#39;homogénéité, peut être utilisé pour tirer une conclusion quant à savoir si deux populations ont la même distribution. Pour calculer la statistique de test pour un test d&#39;homogénéité, suivez la même procédure que pour le test d&#39;indépendance. La valeur attendue de chaque cellule doit être d&#39;au moins cinq pour que vous puissiez utiliser ce test. . Hypothèses H0: Les distributions des deux populations sont les mêmes. . Ha: Les distributions des deux populations ne sont pas les mêmes. . from scipy.stats import chi2_contingency . obs = np.array([[10, 10, 20], [20, 20, 20]]) chi2_contingency(obs) # return [chi, pvalue, df, ndarray] . (2.7777777777777777, 0.24935220877729622, 2, array([[12., 12., 16.], [18., 18., 24.]])) . R&#233;aliser et interpr&#233;ter des tests d&#8217;hypoth&#232;se de variance unique du chi carr&#233; . Au plus d&#39;une seule variance suppose que la distribution sous-jacente est normale. Les hypothèses nulles et alternatives sont énoncées en termes de variance de la population (ou écart type de la population). La statistique du test est: . $ frac{(n- 1)s^2}{σ^2}$ . où : . $n = $ le nombre total de données | $s^2 = $ variance de l&#39;échantillon | $ sigma^2 = $ variance de la population Vous pouvez penser à la variable aléatoire dans ce test. Le nombre de degrés de liberté est df = n- 1. Un test d&#39;une seule variance peut être à droite, à gauche ou bilatéral. L&#39;exemple 11.10 vous montrera comment configurer les hypothèses nulles et alternatives. Les hypothèses nulle et alternative contiennent des déclarations sur la variance de la population. | . from scipy.stats import fligner . a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99] b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05] c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98] stat, p = fligner(a, b, c) p . 0.00450826080004775 .",
            "url": "https://blog.rboyrie.info/statistiques/2021/02/26/Statistics.html",
            "relUrl": "/statistiques/2021/02/26/Statistics.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Quelques Observations Quotidiennes",
            "content": "2021-02-22 . La vision de Copenhague, qui situe le monde quantique dans un autre monde que le monde classique, serait parfait si nous ne voulions pas voir du monde quantique autre chose qu’une vision théorique et mathématique. C’est un monde où les particules se comportent en théorie comme une onde, car rien n’interfère avec leur état. Elles peuvent ainsi être en état de superposition mais sans que nous puissions procéder à une mesure d’une interférence. . Toutefois si nous considérons que la particule est en même temps de nature ondulatoire et corpusculaire, nous obtenons dans ce cas, une vision du monde quantique telle que la décrivent Louis de Broglie et David Bohm et qui s’oppose à la vision de Copenhague. Pour faire simple, l’onde est perçue comme une trame sur laquelle se déplace le corps, un peu comme un surfeur. Mais pour que l’onde agisse sur la particule en permanence et de manière instantanée, quelque-soit la distance entre elles, la transmission devrait se produire à une vitesse supérieure à celle de la lumière, ce qui n’est pas concevable depuis la théorie de la relativité d’Einstein. . Enfin, il peut-être plus intéressant encore de diviser l’univers étudié en autant d’états possibles que peuvent prendre ses particules. En dépliant cette arborescence, nous nous rendons compte qu’il s’agit de duplication de l’univers, un pour chaque état des particules. Le nombre de branches pourra donner le vertige quand les états des particules ne seront pas simplement binaires. . 2021-02-06 . Le grand dilemme de cet avenir vers 2030, de ce futur que nous imaginons infructueux quand nous sommes pessimistes, est dans ce qui représente la prochaine révolution, la physique quantique. Imaginons un seul instant que l’ensemble des positions des électrons dans notre tête soient mesurées par une puce dans notre cerveau relié à un ordinateur quantique par le réseau des réseaux, sans doute dès lors, sera-t-il possible, pour un programme quantique, de deviner nos pensées des minutes qui suivent la mesure. La notion de libre arbitre risque de devenir flou. La fonction d’onde de Schrödinger montre déjà comment se comporte un électron dans son nuage électronique : . $ih frac{ delta psi}{ delta t} = - frac{h^2}{2m} nabla^2 psi+V psi$ . h est une constante, $ psi$ est la fonction d’onde (probabilité de trouvé un électron dans un nuage), V est la description des forces que subit l’électron, et le reste (i, $ frac{ delta}{ delta t}$, $ nabla$ dit nabla) sont des outils mathématiques. Les prédictions seront sans doute plus facile grâce aux ordinateurs quantiques, mais à quel prix pour la liberté de penser ? Les gens seront-ils rangés par paquets bien homogènes ? . 2021-02-02 . Hier je vous parlais d’une idée qui m’avait semblé intéressante, pourquoi, parce que c’est dans l’air du temps et dans mon tempérement également. Mais je sens que je ne suis pas allé assez loin pour atteindre le but que je m’étais fixé qui était celui de dégager mes impressions sur le sujet, donc j’ai trouvé heureusement, ce matin sagement, une énième barrière technologique. Bien que cela bride un peu l’imaginaire, je vais quand même l’évoquer en entrant immédiatement dans le vif du sujet. . C’est tout simplement la perte d’information entre les signaux digitaux et analogiques. La clé de ce sujet est la compression de l’information contenue dans le signal. Il aurait pu sembler regrettable pour un certain ingénieur du son de ma connaissance d’écouter une musique sur un mp3, et pour lui, de plus, il aurait toujours mieux valu un format moins compressé tel que le flac (bien que d’autres formats existent). Mais la vérité c’est que le format mp3 satisfait la majeur parti du public. . Sans doute parce que l’appréciation de la musique ainsi que son interprétation et son réglage n’ont pas la même valeur selon qu’une personne écoute la musique par le ventre ou bien comme une collection de paramètres. L’enjeu de la qualité et de la quantité peut faire débat longtemps, je crois, malheureusement. Qu’en est-il des compositeurs qui entendent les instruments dans leur tête ? Une puce pourrait-elle remplacer cette fonction judicieusement dans leur cerveau ? . 2021-02-01 . La journée commence toujours à la fin lorsque je dois reprendre mes notes pour vous écrire un compte-rendu. Cette fois, j’ai pensé aux puces intégrées dans le cerveau, celles-là mêmes qui pourraient transformer tout novice de la finance en Warren Buffet par une seule opération, certe un peu invasive, mais vous n’aurez rien sans rien. . À quoi bon rêver de ces puces, et surtout, pourrons-nous réellement penser de la même manière une fois qu’elles nous seront implantées ? Une puce ayanc une certaine fréquence d’horloge, alors que le flux de la pensée est irrégulier. Devrons-nous par conséquent apprendre à penser en écoutant un métronome, sans que ça soit un danger majeur pour notre comportement humain. . Les risques seront-ils minimes ou élevés, et pourrons-nous continuer toutes nos activités habituelles ? Tandis que les puces pourraient permettre aux anciens de devenir aussi malin que les petits nouveaux ou en sens contraire, en tout cas, la rotation de la Terre ne s’arrêtera pas de tourner pour si peu… . 2021-01-31 . Encore un autre jour de la période Covid-19 passée dans mon château-fort. En ayant passé des heures à chercher la cause de mon problème pour trouver le remède qui n’était rien de plus que de faire sans faire. Aimer l’eau, l’air, haïr la haine, voilà un résumé de ce petit manuel du terrien. . 2021-01-30 . Les aiguilles ont fait un tour de cadran et je goutte une nouvelle liberté d’action pour organiser un nouveau projet professionnel, après la restructuration de mon service pro, j’entre dans une période de recherche d’une nouvelle entreprise pour un poste à l’identique. Je suis confiant car je ne suis pas le seul à dire que la Data Science et l’IA sont des domaines qui ont un avenir, mais, je l’espère pour moi, aussi en France ! Car malgré les autres grandes puissances et la diversification du hardware qui ira en augmentant (tel que nous pouvons l’imaginer), nous avons des Grands Groupes, et une French Tech avec ses startups ou même ses licornes. . 2021-01-29 . Demain j’aurai sans doute terminé mon travail sur les outils statistiques que je m’étais fixé le mois dernier. Connaître bientôt la satisfaction du travail accompli me permet de garder espoir dans le futur tout en pensant à autre chose qu’au stress hydrique que connaissent déjà plusieurs villes sur Terre. .",
            "url": "https://blog.rboyrie.info/vie%20priv%C3%A9e/2021/01/29/Daily-Observations.html",
            "relUrl": "/vie%20priv%C3%A9e/2021/01/29/Daily-Observations.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Signification statistique",
            "content": "Humilit&#233; . Difficile de parler de vérité absolue dans un résultat statistique, le scientifique reconnaît presque toujours que son résultat d’analyse dépend de la quantité et de la qualité de ses data. Certains pourront y voir un problème de culture scientifique, entre le statisticien, les big data… gageons que l’adage &quot;Vérité en deçà des Pyrénées, mensonges au-delà&quot; puisse illustrer à merveille les représentations de l’absence d’une vérité unique et immuable, ne serait-ce que parce que la sagesse nous enseigne que tout est éphémère dans l’univers. La chance et l’aléatoire prennent beaucoup de place dans les statistiques et valider une hypothèse nécessite la réduction maximale de leur impact. . Observation . De la fabrication d’une hypothèse à sa validation, tout scientifique se passionnera en se représentant les effets et les causes d’un événement. Les mathématiques statistiques sont un moyen de démontrer qu’entre l’hypothèse sans cause, et l’hypothèse avec cause, il y a une signification statistique, en-deçà de laquelle il est possible de conclure de la valeur de l’effet sur la cause. Mais ce n’est pas la panacé, et bien des erreurs se produisent dans la pratique (un exemple est l’effet placebo) et malgré leur origine théorique, la signification statistique est juste ce que l’on fait de plus scientifique actuellement. Si l’on se penche sur les expériences autrement plus difficile à produire en physique quantique, ou les conditions des expériences influent énormément sur les résultats, cela entraîne que la reproductibilité est parfois impossible et c’est très ennuyeux. Comment alors confirmer les résultats d’un chercheur. Toutefois, on arrête pas la marche du progrès ainsi, et rester les bras croisés et ne croire aucun rapport scientifique demeure de l’ordre de l’obscurantisme, et rien ne saurait remplacer le pragmatisme de la science, ne serait-ce que pour les vaccins. De plus, la théorie statistique permet aussi de prédire, parfois en extrapolant avec risque, néanmoins, les observations empiriques permettent bien sûr de penser a posteriori, le seul hic, c’est que pour valider une hypothèse, il est nécessaire d’essayer d’observer l’expérience un grand nombre de fois. . Multiplier les essais . Pour démontrer que les lois statistiques peuvent décrire la vérité, il est nécessaire de reproduire les conditions de l’expérience un grand nombre de fois. Dans l’expérience du lancé de 2 dés en simultané, par exemple, nous savons qu’il ne peut pas y avoir de corrélation entre le résultat des deux dés, à moins d’un facteur de chance ou d’un aléas, et celui-ci diminue plus le nombre de lancés augmente. Pour démontrer que plus le nombre d’essais est grand et plus le facteur chance et d’aléas disparaissent, nous allons représenter la distribution des corrélations des résultats des lancés. Nous savons que si le nombre d’essais est infini, la corrélation tend vers 0 par bon sens, mais nous allons créer la densité de probabilité en quelques lignes de programme Python, et nous découvrirons cette forme de cloche si particulière de Gauss. . !pip install numpy . Chargement des modules . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns . Nous créons la fonction d’expériences pour créer la distribution . def dices(N=10, k=500): &quot;&quot;&quot;Renvoie une distribution corrélation entre N lancés de deux dés pour k réalisations d’expérience. &quot;&quot;&quot; np.random.seed(42) # 42 comme graine d’aléas cf. œuvre de Douglas Adams qui l’a popularisé comme # la réponse à l’ultime question de la vie, de l’Univers et de toute chose. N = N k = k distrib = [] for i in range(k): XY = np.random.randint(1,6,(2,N)) rho = round(np.corrcoef(XY)[0,1],2) distrib.append(rho) return distrib . plt.figure(figsize=(6,3), dpi= 80) # Dimensionnement du tableau kwargs = dict(hist_kws={&#39;alpha&#39;:.6}, kde_kws={&#39;linewidth&#39;:2})# Dictionnaire de gestion colorimétrique, épaisseur sns.distplot(dices(10,5), color=&quot;dodgerblue&quot;, label=&quot;Compact&quot;, **kwargs) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x268ec11b040&gt; . Ici, visuellement, il est difficile d’être en mesure de faire de l’inférence statistique car la forme de cloche, que l’on approche très souvent à la loi Normale, n’est pas visible. Nous allons refaire l’expérience en augmentant le nombre d’expériences à 2000 fois. . plt.figure(figsize=(6,3), dpi= 80) # Dimensionnement du tableau kwargs = dict(hist_kws={&#39;alpha&#39;:.6}, kde_kws={&#39;linewidth&#39;:2})# Dictionnaire de gestion colorimétrique, épaisseur sns.distplot(dices(10, 2000), color=&quot;dodgerblue&quot;, label=&quot;Compact&quot;, **kwargs) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x268ec127700&gt; . En conclusion, nous apercevons que nous retrouvons une courbe en forme de cloche dont le pic indique que la corrélation n’existe pas entre les 10 lancés de deux dés. À partir de cette courbe de Gauss, nous appuyant sur la loi Normale, nous allons pouvoir exprimer un intervalle de confiance à l’intérieur duquel, nous pourrons circonscrire tout résultat des expériences reproduisant nos conditions expérimentales. Grâce à cette certitude, le scientifique peut détecter les cas qui sortent de l’ordinaire, qui déconstruisent une hypothèse en favorisant une autre, par le biais de la signification statistique (notée $ alpha$ par convention et souvent égale à 5%). .",
            "url": "https://blog.rboyrie.info/math/statistiques/2020/12/16/Signification-statistique.html",
            "relUrl": "/math/statistiques/2020/12/16/Signification-statistique.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Matrices",
            "content": "Domaine . Les matrices sont des objets mathématiques que l’on regroupe dans l’Algèbre Linéaire. Elles permettent d’exprimer des vecteurs dans des espaces à n dimensions. Un vecteur peut être une position, votre géolocalisation sur la Terre, ou tout simplement une information telle que votre rythme cardiaque ou votre revenu. C’est pour cela que notre monde à 3 dimensions spatiales et extensible peuvant recevoir autant de dimension que nécessaire pour décrire la position d’une position appelé vecteur dans un espace à n dimension. . Op&#233;rateur . Quels sont les opérateurs des matrices : . Le produit scalaire : équivalent d’une projection d’un premier vecteur, sur le second. | Le produit vectoriel : c’est la recherche du vecteur perpendiculaire à un plan formé par deux vecteurs, dont la norme est aussi l’aire de ce plan. | La norme : la racine de la somme des carrés des longueurs d’un vecteur sur chaque direction de l’espace à n dimension. | Le déterminant : c’est une caractéristique quantitative des normes de vecteurs dans un espace à n dimensions. Si n vaut 2 ou 3, c’est l’aire ou le volume. | Le vecteur unitaire : c’est un vecteur colinéaire et de même direction qu’un autre, mais sa norme vaut 1. | Le valeur propre : coefficient d’étirement d’un vecteur propre, d’une application linéaire dans un espace vectoriel. | La vecteur propre : vecteur qui est juste étiré ou raccourci lors d’une transformation linéaire dans un espace vectoriel. | Utilisation de Python pour chaque op&#233;rateur . Pour réaliser nos calculs nous aurons besoin de la bibliothèque numpy afin de construire nos tableaux de valeurs. Pour l’installation dans Jupyter-notebook, nous pouvons utiliser la commande suivante : . !pip install scipy numpy sympy . Chargement des biblioth&#232;ques . import numpy as np from numpy import linalg as LA from numpy.linalg import eig from sympy import * . Produit scalaire . ### Produit scalaire: produit de deux vecteurs f = np.array([1,2,4,6,7]) g = np.array([4,5,3,8,3]) ### 1*4+2*5+4*3+6*8+7*3 np.dot(f, g) . 95 . Dans l’étude du produit vectoriel et de la norme ci-dessous, nous considérons semblables les représentations géométriques et algébriques de vecteurs. Cela nous permet de dire que la norme d’un vecteur h par le produit vectoriel de deux vecteurs f, g est égale à l’aire du plan formé par ces deux derniers vecteurs. . Produit vectoriel . ### Produit vectoriel équivaut au produit extérieur: f x g = f ∧ g f = np.array([3,5,2]) g = np.array([5,7,4]) ### i(5*4-7*2)+j(5*2-3*4)+k(3*7-5*5) np.cross(f, g) . array([ 6, -2, -4]) . Norme . ### Produit vectoriel équivaut au produit extérieur: f x g = f ∧ g soit le vecteur h f = np.array([3,5,2]) g = np.array([5,7,4]) ### i(5*4-7*2)+j(5*2-3*4)+k(3*7-5*5) h = np.cross(f, g) ### La norme de h vaut le produit extérieur f ∧ g LA.norm(h,1) . 12.0 . Le d&#233;terminant . a = np.array(([1,2],[-3,4])) int(np.linalg.det(a)) . 10 . Nous pouvons tout à fait utiliser plutôt le module sympy et faire le calcul comme suivant. . M = Matrix([[1,2],[-3,4]]) M . $ displaystyle left[ begin{matrix}1 &amp; 2 -3 &amp; 4 end{matrix} right]$ M.det() . $ displaystyle 10$ Le vecteur unitaire . Le vecteur unitaire a une magnitude de 1, sa formule est $ hat{v} = frac{1}{ lvert lvert v rvert rvert} cdot v$ . . v = np.arange(3) v_hat = v / np.linalg.norm(v) v_hat . array([0. , 0.4472136 , 0.89442719]) . La valeur propre et le vecteur propre . Dans une application linéaire, un vecteur propre est la transformation linéaire telle celle de la multiplication d’un vecteur par une valeur propre (scalaire). . $T(v)= lambda v$ . En somme, pour une transformation donnée, c’est les vecteurs qui sont sur la ou les ligne(s) de transformation(s) et pour qui tout vecteur se trouvant dessus sera soit étiré soit réduit mais dont la direction ne changera pas. . a = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]]) values , vectors = eig(a) print(values) print(vectors) . [11. 1. 2.] [[ 0. 0. 1. ] [ 0.4472136 0.89442719 0. ] [ 0.89442719 -0.4472136 0. ]] . De même, avec le module python sympy, nous pouvons déclarer une matrice M et exécuter les calculs. . M = Matrix([[2, 0, 0], [0, 3, 4], [0, 4, 9]]) M . $ displaystyle left[ begin{matrix}2 &amp; 0 &amp; 0 0 &amp; 3 &amp; 4 0 &amp; 4 &amp; 9 end{matrix} right]$ M.eigenvals() . {11: 1, 2: 1, 1: 1} . M.eigenvects() . [(1, 1, [Matrix([ [ 0], [-2], [ 1]])]), (2, 1, [Matrix([ [1], [0], [0]])]), (11, 1, [Matrix([ [ 0], [1/2], [ 1]])])] . M = Matrix([[2, 4], [3, 13]]) M . $ displaystyle left[ begin{matrix}2 &amp; 4 3 &amp; 13 end{matrix} right]$ M.eigenvals() . {14: 1, 1: 1} .",
            "url": "https://blog.rboyrie.info/math/matrices/2020/12/10/Matrix.html",
            "relUrl": "/math/matrices/2020/12/10/Matrix.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Introducing fastlinkcheck",
            "content": ". Motivation . Recently, fastai has been hard at work improving and overhauling nbdev, a literate programming environment for python. A key feature of nbdev is automated generation of documentation from Jupyter notebooks. This documentation system adds many niceties, such as the following types of hyperlinks automatically: . Links to source code on GitHub. | Links to both internal and external documentation by introspecting variable names in backticks. | . Because documentation is so easy to create and maintain in nbdev, we find ourselves and others creating much more of it! In addition to automatic hyperlinks, we often include our own links to relevant websites, blogs and videos when documenting code. For example, one of the largest nbdev generated sites, docs.fast.ai, has more than 300 external and internal links at the time of this writing. . The Solution . Due to the continued popularity of fastai and the growth of new nbdev projects, grooming these links manually became quite tedious. We investigated solutions that could verify links for us automatically, but were not satisfied with any existing solutions. These are the features we desired: . A platform independent solution that is not tied to a specific static site generator like Jekyll or Hugo. | Intelligent introspection of external links that are actually internal links. For example, if we are building the site docs.fast.ai, a link to https://docs.fast.ai/tutorial should not result in a web request, but rather introspection of the local file system for the presence of tutorial.html in the right location. | Verification of any links to assets like CSS, data, javascript or other files. | Logs that are well organized that allow us to see each broken link or reference to a non-existent path, and the pages these are found in. | Parallelism to verify links as fast as possible. | Lightweight, easy to install with minimal dependencies. | . We tried tools such as linkchecker and pylinkvalidator, but these required your site to be first be hosted. Since we wanted to check links on a static site, hosting is overhead we wanted to avoid. . This is what led us to create fastlinkcheck, which we discuss below. . Note: For Ruby users, htmlproofer apperas to provide overlapping functionality. We have not tried this library. . A tour of fastlinkcheck . For this tour we will be referring to the files in the fastlinkcheck repo. You should clone this repo in the current directory in order to follow along: . git clone https://github.com/fastai/fastlinkcheck.git cd fastlinkcheck . Cloning into &#39;fastlinkcheck&#39;... remote: Enumerating objects: 135, done. remote: Counting objects: 100% (135/135), done. remote: Compressing objects: 100% (98/98), done. remote: Total 608 (delta 69), reused 76 (delta 34), pack-reused 473 Receiving objects: 100% (608/608), 1.12 MiB | 10.47 MiB/s, done. Resolving deltas: 100% (302/302), done. . Installation . You can install fastlinkcheck with pip: . pip install fastlinkcheck . Usage . After installing fastlinkcheck, the cli command link_check is available from the command line. We can see various options with the --help flag. . link_check --help . usage: link_check [-h] [--host HOST] [--config_file CONFIG_FILE] [--pdb] [--xtra XTRA] path Check for broken links recursively in `path`. positional arguments: path Root directory searched recursively for HTML files optional arguments: -h, --help show this help message and exit --host HOST Host and path (without protocol) of web server --config_file CONFIG_FILE Location of file with urls to ignore --pdb Run in pdb debugger (default: False) --xtra XTRA Parse for additional args (default: &#39;&#39;) . From the root of fastlinkcheck repo, We can search the directory _example/broken_links recursively for broken links like this: . link_check _example/broken_links . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Specifying the --host parameter allows you detect links that are internal by identifying links with that host name. External links are verified by making a request to the appropriate website. On the other hand, internal links are verified by inspecting the presence and content of local files. . We must be careful when using the --host argument to only pass the host (and path, if necessary) without the protocol. For example, this is how we specify the hostname if your site&#39;s url is http://fastlinkcheck.com/test.html: . link_check _example/broken_links --host fastlinkcheck.com . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . We now have one less broken link as there is indeed a file named test.html in the root of the path we are searching. However, if we add a path to the end of --host , such as fastlinkcheck.com/mysite the link would again be listed as broken because _example/broken_links/mysite/test.html does not exist: . link_check _example/broken_links --host fastlinkcheck.com/mysite . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . You can ignore links by creating a text file that contains a list of urls and paths to ignore. For example, the file _example/broken_links/linkcheck.rc contains: . cat _example/broken_links/linkcheck.rc . test.js https://www.google.com . We can use this file to ignore urls and paths with the --config_file argument. This will filter out references to the broken link /test.js from our earlier results: . link_check _example/broken_links --host fastlinkcheck.com --config_file _example/broken_links/linkcheck.rc . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Finally, if there are no broken links, link_check will not return anything. The directory _example/no_broken_links/ does not contain any HTML files with broken links: . link_check _example/no_broken_links . No broken links found! . Python . You can also use these utilities from python instead of the terminal. Please see these docs for more information. . Using link_check in GitHub Actions . The link_check CLI utility that is installed with fastlinkcheck can be very useful in continuous integration systems like GitHub Actions. Here is an example GitHub Actions workflow that uses link_check: . name: Check Links on: [workflow_dispatch, push] jobs: check-links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 - name: check for broken links run: | pip install fastlinkcheck link_check _example . We can a few more lines of code to open an issue instead when a broken link is found, using the gh cli: . ... - name: check for broken links run: | pip install fastlinkcheck link_check _example 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; [[ -s err ]] &amp;&amp; gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -R &quot;yourusername/yourrepo&quot; . We can extend this even further to only open an issue when another issue with a specific label isn&#39;t already open: . ... - name: check for broken links run: | pip install fastlinkcheck link_check &quot;docs/_site&quot; --host &quot;docs.fast.ai&quot; 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; if [[ -z $(gh issue list -l &quot;broken-link&quot;)) &amp;&amp; (-s err) ]]; then gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -l &quot;broken-link&quot; -R &quot;yourusername/yourrepo&quot; fi . See the GitHub Actions docs for more information. . Resources . The following resources are relevant for those interested in learning more about fastlinkcheck: . The fastlinkcheck GitHub repo | The fastlinkcheck docs | .",
            "url": "https://blog.rboyrie.info/fastlinkcheck/",
            "relUrl": "/fastlinkcheck/",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "One-way ANOVA",
            "content": "O&#249; s&#8217;utilise cette m&#233;thode . Nous trouvons cette méthode d’analyse de la variance dans les sciences comme la psychologie, les sciences sociales, les sciences naturelles. Elle permet l’analyse d’une variable indépendante dans plusieurs groupes : un sociologue pourra mesurer le niveau de salaire d’une personne en fonction de son niveau d’étude, un consommateur pourra comparer la consommation moyenne de véhicules suivant le modèle… Pour mesurer plusieurs groupes, les statisticiens ont développé la méthode d’analyse de la variance ou ANOVA. La plus simple, qui analyse une seule variable indépendante est la one-way ANOVA. . Principe . Le but de la méthode one-way ANOVA est de trouver s’il existe une différence statistiquement significative dans les moyennes de plusieurs groupes. La méthode utilise la variance pour vérifier que les moyennes sont semblables. . Hypoth&#232;ses de travail . Chaque échantillon est pris d’une population dont la distribution est considérée comme suivant une loi Normale. | L’échantillonnage est aléatoire, et le tirage de chaque groupe est indépendent. | Les populations des groupes ont un écart type équivalent ou égal. | Le facteur est une variable de catégorie | La réponse est une variable numérique | Test d&#8217;hypoth&#232;se . L’hypothèse nulle est le cas dans lequel toutes les moyennes sont égales. L’hypothèse alternative est qu’au moins un couple de moyennes n’est pas équivalent. . Calcul du test d&#8217;hypoth&#232;se . Pour réaliser le calcul, nous utilisons la distribution F. Cette statistique est un ratio (un rapport, une fraction) comprenant deux degrés de liberté, un pour le numérateur, l’autre pour le dénominateur. . Note :La distribution F dérive du t-test et de la distribution Student, qui est simplement l’élévation au carré de celle-là. Il est préférable d’utiliser la méthode one-way ANOVA quand on veut étudier plusieurs groupes au-lieu que de réaliser de multiple t-test à cause du risque $ alpha$ qui est le rejet de l’hypothèse nulle alors qu’elle est vraie. . Le calcul de Ratio-F . $F = frac{MS_{between}}{MS_{within}}$ La valeur de F est le rapport entre la moyenne de la variance au carré entre les groupes, et la moyenne de la variance au carré dans les groupes (dû à l’échantillonage). . $MS_{between} = frac{SS_{between}}{df_{between}} = frac{SS_{between}}{k - 1}$ avec $k$ comme nombre de groupe . $MS_{within} = frac{SS_{within}}{df_{within}} = frac{SS_{within}}{n-k}$ avec $n$ comme somme de la quantité d’observations pour tous les groupes. . Quand les groupes ont la même taille, la valeur de F revient à : . $F= frac{n cdot {s_{ bar{x}}}^{2}}{s^{2}}$ . Nous détaillerons plus le calcul dans l’exemple suivant. . Importation des modules scientifiques . Pour réaliser nos calculs nous aurons besoin de la bibliothèque scientifique de python scipy et également de numpy pour construire nos tableaux de valeurs. Pour l’installation dans Jupyter-notebook, nous pouvons utiliser la commande suivante : . !pip install scipy numpy . Requirement already satisfied: scipy in c: programdata anaconda3 lib site-packages (1.5.0) Requirement already satisfied: numpy in c: programdata anaconda3 lib site-packages (1.18.5) . Nous importons la méthode pour la méthode one-way ANOVA, la fonction de densité de F et le module numpy . from scipy.stats import f_oneway from scipy.special import fdtrc import numpy as np . Exemple . Nous collectons des données du supermarché du coin afin de réaliser une analyse de la variance sur trois produits : des fruits, des légumes et des pains. Nous collectons nos données de manière aléatoire à partir des rayons de notre supermarché en prenant le prix au kilos de huit produits non contigus dans chacun des rayons. Nous retrouvons nos 3 catégories et les valeurs numériques (prix par kilogramme) comme suivant : . fruits = np.array([1.99, 3.05, 2.60, 1.45, 2.99, 2.91, 2.72, 3.15]) legumes = np.array([1.25, 1.75, 1.40, 1.66, 2.20, 2.70, 3.00, 1.99]) pains = np.array([0.90, 1.25, 1.10, 1.55, 2.20, 3.00, 3.05, 1.05]) . L’hypothèse nulle $H_{0}$ est que les moyennes des prix de ces trois catégories de produit ne sont pas statistiquement différentes de manière significative au vue de notre jeu de données. Nous pourrons prendre une pvalue de 5%, ce qui représente une certitude de 95% quant à notre résultat, en statistique. . Pour les fruits, les légumes, et les pains, nous avons les valeurs statistiques suivantes : . fruits_mean = fruits.mean() fruits_var = fruits.var() fruits_cnt = len(fruits) fruits_stat = &#39;La moyenne des fruits est &#39; + f&#39;{fruits_mean:0.3}&#39; + &#39;, La variance est &#39; + f&#39;{fruits_var:0.2}&#39; + &#39;, Le nombre d’observations est &#39; + f&#39;{fruits_cnt}&#39; fruits_stat . &#39;La moyenne des fruits est 2.61, La variance est 0.31, Le nombre d’observations est 8&#39; . legumes_mean = legumes.mean() legumes_var = legumes.var() legumes_cnt = len(legumes) legumes_stat = &#39;La moyenne des légumes est &#39; + f&#39;{legumes_mean:0.3}&#39; + &#39;, La variance est &#39; + f&#39;{legumes_var:0.2}&#39; + &#39;, Le nombre d’observations est &#39; + str(legumes_cnt) legumes_stat . &#39;La moyenne des légumes est 1.99, La variance est 0.33, Le nombre d’observations est 8&#39; . pains_mean = pains.mean() pains_var = pains.var() pains_cnt = len(pains) pains_stat = &#39;La moyenne des pains est &#39; + f&#39;{pains_mean:0.3}&#39; + &#39;, La variance est &#39; + f&#39;{pains_var:0.2}&#39; + &#39;, Le nombre d’observations est &#39; + str(pains_cnt) pains_stat . &#39;La moyenne des pains est 1.76, La variance est 0.67, Le nombre d’observations est 8&#39; . Estimation du ratio-F, par la distribution Fischer . Pour parvenir à notre résultat, nous ferons plusieurs calculs. . Les degrés de libertés sont (pour n = 24 , k = 3) : . $df_{num} = 3 - 1$ soit 2 . et . $df_{denom} = 24 - 3$ soit 21 . La valeur de F que nous cherchons est $F_{2,21}$ soit dans la table des valeurs de la distribution F : 3.46 . alldata = np.concatenate([[fruits,legumes,pains]]) k = len(alldata) # Nombre de catégories ...2 N = alldata.shape[0] * alldata.shape[1] # Nombre d’observations ...24 n = alldata.shape[1] # Nombre d’observations dans chaque catégorie ...8 . df_num = k - 1 df_denom = N - k . $SS_{between} = frac{ sum{x^2}}{n} - frac{( sum{x})^2}{N}$ . SS_between = (np.array([fruits.sum()**2, legumes.sum()**2, pains.sum()**2]).sum()/n ) - (np.array([fruits.sum(), legumes.sum(), pains.sum()]).sum()**2 /N) . $SS_{tot} = sum{x^2} - frac{( sum{x})^2}{N}$ . SS_tot = (np.array([fruits**2, legumes**2, pains**2]).sum())- (np.array([fruits, legumes, pains]).sum()**2 /24) . $SS_{within} = SS_{tot}- SS_{between}$ . SS_within = SS_tot - SS_between . MS_between = SS_between / df_num . MS_within = (SS_within) / df_denom . F = MS_between / MS_within F . 3.0596582667896874 . $pvalue = P(F&gt; 3.46) = 0.0682$ . pvalue = fdtrc(df_num, df_denom, F) pvalue . 0.06821436223117332 . En conclusion, d’après nos données, nous sommes sûr à 95% que l’hypothèse nulle $H_{0}$ doit-être rejetée. . La même conclusion peut être calculée, plus rapidement, avec la méthode f_oneway() du module scipy. . f_oneway(fruits, legumes, pains) . F_onewayResult(statistic=3.059658266789707, pvalue=0.06821436223117218) . Restituer graphiquement le r&#233;sultat . Pour observer nos résultats et comprendre visuellement le résultat statistique qui indique que les moyennes sont différentes, nous allons importer deux autres modules en Python pour dessiner nos données. . import pandas as pd import matplotlib.pyplot as plt . df = pd.DataFrame(alldata.T, columns=[&#39;Fruits&#39;,&#39;Legumes&#39;,&#39;Pains&#39;]) # Dessiner un graphique de boîte à moustache boxplot = df.boxplot( figsize=(12, 8)) # Sauvegarder vos boîtes à moustaches plt.savefig(&#39;image.png&#39;) . Grâce à ce graphique qui représente en plus, très bien les quartiles, nous pouvons voir que les fruits contiennent une valeur aberrante, un outsider, très loin du reste du paquet de l’échantillon. .",
            "url": "https://blog.rboyrie.info/statistiques/anova/2020/11/12/One-way-ANOVA.html",
            "relUrl": "/statistiques/anova/2020/11/12/One-way-ANOVA.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://blog.rboyrie.info/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://blog.rboyrie.info/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://blog.rboyrie.info/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://blog.rboyrie.info/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.rboyrie.info/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.rboyrie.info/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}