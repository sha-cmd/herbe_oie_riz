{
  
    
        "post0": {
            "title": "Math en Latex",
            "content": "Greek letters . alpha $ alpha$ A nu N $ nu$ $N$ $ beta$ $B$ xi Xi $ xi$ $ Xi$ . gamma Gamma $ gamma$ $ Gamma$ o O $o$ $O$ delta Delta } delta Delta pi Pi } pi Pi epsilon varepsilon E ;} epsilon varepsilon E rho varrho P ;} rho varrho P zeta Z} zeta Z sigma ,! Sigma ;} sigma Sigma eta H} eta H tau T} tau T theta vartheta Theta } theta vartheta Theta upsilon Upsilon } upsilon Upsilon iota I} iota I phi varphi Phi } phi varphi Phi kappa K} kappa K chi X} chi X lambda Lambda ;} lambda Lambda psi Psi } psi Psi mu M} mu M omega Omega } omega Omega Arrows leftarrow } leftarrow Leftarrow } Leftarrow rightarrow } rightarrow Rightarrow ;} Rightarrow leftrightarrow } leftrightarrow rightleftharpoons } rightleftharpoons uparrow } uparrow downarrow } downarrow Uparrow ;} Uparrow Downarrow } Downarrow Leftrightarrow ;} Leftrightarrow Updownarrow } Updownarrow mapsto } mapsto longmapsto ;} longmapsto nearrow } nearrow searrow } searrow swarrow } swarrow nwarrow } nwarrow leftharpoonup } leftharpoonup rightharpoonup } rightharpoonup leftharpoondown } leftharpoondown rightharpoondown } rightharpoondown Miscellaneous symbols infty ; ;} infty forall ;} forall Re } Re Im } Im nabla } nabla exists } exists partial } partial nexists } nexists emptyset } emptyset varnothing ;} varnothing wp } wp complement } complement neg } neg cdots } cdots square } square surd } surd blacksquare } blacksquare triangle } triangle Binary Operation/Relation Symbols times } times times } times div } div cap } cap cup } cup neq ;} neq leq } leq geq } geq in } in perp ;} perp notin } notin subset } subset simeq } simeq approx } approx wedge } wedge vee } vee oplus ;} oplus otimes } otimes Box } Box boxtimes } boxtimes . equiv } equiv cong } cong .",
            "url": "https://blog.rboyrie.info/statistiques/2021/03/05/Latex_Math.html",
            "relUrl": "/statistiques/2021/03/05/Latex_Math.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistiques Bayesiennes",
            "content": "Th&#233;or&#232;me de Bayes : . Voici tout se dont vous devez savoir à propos des statistiques (R. Feynman) : . $P( theta|y) = frac{P(y| theta) cdot P( theta)}{P(y)}$ . D’après la règle des produits : . $P( theta,y) = P( theta|y)P(y)$ . ou . $P( theta,y) = P(y| theta)P( theta)$ . Ce qui équivaut en les combinant à : . $P( theta|y)P(y) = P(y| theta)P( theta)$ . Si nous remplaçons $ theta$ par l’hypothèse et y par les data, le théorème de Bayes nous dit comment calculer la probabilité de notre hypothèse sachant nos données. L’hypothèse est transformé en probabilité en utilisant des distributions de probabilités. . $P( theta)$ : Primauté, les statisticiens la qualifie de subjective. . $P(y| theta)$ : La certitude, le modèle de l’échantillon, le modèle statisque ou le modèle. . $P( theta|y)$ : Le posterieur de la distribution, c’est une distribution de probabilité pour notre paramètre $ theta$ et non une simple valeur. . $P(y)$ : Probabilité marginale, ou évidence. Peut-être ignoré par les débutants, en ne la considérant que comme une valeur relative et non pas comme une valeur absolue. . &#192; quoi sert-il . Les probabilités sont des outils pour mesurer l’incertitude à propos de paramètres, et le théorème de Bayes est le méchanisme pour mettre à jour correctement ces probabilités à la lumière de nouvelles données, avec l’espoir de pouvoir réduire l’incertitude. .",
            "url": "https://blog.rboyrie.info/statistiques/2021/03/05/Bayesian-Statistics.html",
            "relUrl": "/statistiques/2021/03/05/Bayesian-Statistics.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Statistiques",
            "content": "Ind&#233;pendance . Deux événements sont indépendants si ce qui suit est vrai : | . $P(A|B) = P(A) cdot P(B|A) = P(B) cdot P(A ET B)$ . $P(A ET B) = P(A) cdot P(B)$ . Quand P(B) ne vaut ni 0, ni 1, les deux événements A et B sont indépendant si : . $P(A) = P(A|B)$ . Deux événements sont indépendants si la connaissance que l’un s’est produit n’affecte pas la chance que l’autre se produise. | . Prouver l&#8217;ind&#233;pendance de deux variables (1 condition parmi les 3 suivantes suffit) : . Si deux événements ne sont PAS indépendants, alors nous disons qu’ils sont indépendants. | . L’échantillonage peut être fait avec replacement ou sans replacement : . Avec replacement : Lorsque l’échantillonage est réalisé avec replacement, les événements sont considérés comme indépendants, ce qui signifie que les résultats du premier ne change pas le résultat du deuxième prélèvement. . | Sans replacement : Lorsque l’échantillonage est réalisé sans replacement, chaque membre d’une population peut être choisi uniquement une fois. Dans ce cas les probabilités pour le deuxième choix sont affectées par le résultat du premier choix. Les événements sont considérés comme dépendants ou non indépendants. . | Distribution Binomiale . Il existe trois caractéristiques de la distribution binomiale. . 1-. Il existe un nombre fixe d’essais. La lettre $n$ désigne le nombre d’essais. C’est le cas dans la répétition d’une expérience. . Il n’y a que deux résultats possibles, «succés» ou «échecs», pour chaque essais. La lettre $p$ indique la probabilité de succés sur un essai, et $q$ indique la probabilité d’échec sur un essai. $p + q = 1$ . | Les essais $n$ sont indépendants et sont répétés dans des conditions identiques. Comme les essais sont indépendants, les probabilités $p$ et $q$ sont les mêmes pour chaque essais. . | La loi de comportement est $f(k) = binom{n}{k}p^k(1-p)^{n-k}$ . from scipy.stats import binom . n, p, k, loc = 5, 0.4, 3, 0# k est l’essais qui nous intéresse pour la mesure binom.pmf(k - loc, n, p) . 0.2304 . Distribution G&#233;om&#233;trique . Il existe trois caractéristiques principales d’une expérience géométrique. . Il y a un ou plusieurs essais de Bernoulli avec tous les échecs sauf le dernier, qui est un succés. En d’autres termes, vous répétez ce que vous faites jusqu’au premier succés. Ensuite, vous arrêtez. . | En théorie le nombre d’essais peut durer indéfiniment. Il doit y avoir au moins un essai. . | La probabilité $p$ d’un succés et la probabilité $q$ d’un échec sont les mêmes pour chaque essai. $p + q = 1$ et $q = 1 - p$. La probabilité d’obtenir un 3 en lançant un dé est toujours de $ frac{1}{6}$. . | La probabilité d’obtenir un 3 au cinquième lancé est : $( frac{5}{6}) cdot( frac{5}{6}) cdot( frac{5}{6}) cdot( frac{5}{6}) cdot( frac{1}{6}) = 0.080375514403293$ . La loi de comportement est $f(k) = (1 - p)^{(k-1)}p$ . from scipy.stats import geom . p, k, loc = 1/6, 5, 0 # loc sert à décalé la position de départ du décompte de la distribution. geom.pmf(k - loc, p) . 0.08037551440329219 . Distribution Hyperg&#233;om&#233;trique . Il existe cinq caractéristiques d’une expérience hypergéométrique . Vous prélevez des échantillons de deux groupes. . | Vous êtes concerné par un groupe d’intérêt, appelé le premier groupe. . | Vous échantilloner sans remplacement à partir des groupes combinés. Par exemple, vous voulez choisir une équipe de softball parmi un groupe combiné de 11 hommes et 13 femmes. L&#39;équipe se compose de dix joueurs. . | Chaque prélèvement n&#39;est pas indépendant, car l&#39;échantillonnage est sans remplacement. Dans l&#39;exemple du softball, la probabilité de choisir une femme en premier est de $ frac{13}{24}$. La probabilité de choisir un homme en second est $ frac{11}{23}$ si une femme a été choisie en premier. C&#39;est $ frac{10}{23}$ si un homme a été choisi en premier. La probabilité du deuxième choix dépend de ce qui s&#39;est passé lors du premier choix. . | Vous ne traitez pas avec les essais de Bernoulli. Les résultats d&#39;une expérience hypergéométrique correspondent à une distribution de probabilité hypergéométrique. La variable aléatoire $X = $le nombre d&#39;items du groupe d&#39;intérêt. . | La probabilité de masse est : $p(k, M, n, N) = frac{ binom{n}{k} binom{M - n}{N - k}}{ binom{M}{N}}$ . le coefficient binomial est : $ binom{n}{k}≡ frac{n!}{k!(n - k)!}$ . from scipy.stats import hypergeom . k, M, n, N = 3, 20, 7, 12 # k nombre d’éléments du groupe d’intérêt tirés, M nombre d’éléments combinés, # n nombre d’éléments du premier groupe, N nombre d’éléments tirés hypergeom.pmf(k - loc, M, n, N) . 0.19865841073271373 . Distribution de Poisson . Il existe deux caractéristiques principales d’une expérience de Poisson. . La distribution de probabilité de Poisson donne la probabilité qu&#39;un certain nombre d&#39;événements se produisent dans un intervalle de temps ou d&#39;espace fixe si ces événements se produisent avec un taux moyen connu et indépendamment du temps écoulé depuis le dernier événement. Par exemple, un éditeur de livre pourrait être intéressé par le nombre de mots mal orthographiés dans un livre particulier. Il se peut qu&#39;en moyenne, il y ait cinq mots mal orthographiés sur 100 pages. L&#39;intervalle correspond aux 100 pages. . | La distribution de Poisson peut être utilisée pour approcher le binôme si la probabilité de succès est &quot;petite&quot; (comme 0,01) et le nombre d&#39;essais est &quot;grand&quot; (comme 1 000). $n$ est le nombre d&#39;essais et $p$ la probabilité d&#39;un &quot;succès&quot;. La variable aléatoire $X =$ le nombre d&#39;occurrences dans l&#39;intervalle d&#39;intérêt. . | La probabilité de masse est : $f(k) = exp(- mu) frac{ mu^{k}}{k!}$ pour $k ge0$ . from scipy.stats import poisson . mu, k = 0.05, 100 poisson.cdf(k - loc, mu) . 1.0 . La distribution uniforme . La distribution uniforme est une distribution de probabilité continue et concerne les événements qui sont également susceptibles de se produire. Lorsque vous travaillez sur des problèmes qui ont une distribution uniforme, veillez à noter si les données incluent ou excluent les points de terminaison. . from scipy.stats import uniform . x, scale, loc =9, 24, 0 uniform.pdf(x, loc, scale) . 0.041666666666666664 . La distribution exponentielle . La distribution exponentielle concerne souvent le temps écoulé jusqu&#39;à ce qu&#39;un événement spécifique se produise. . Par exemple, le temps (à partir de maintenant) jusqu&#39;à ce qu&#39;un tremblement de terre se produise a une distribution exponentielle. . D&#39;autres exemples incluent la durée, en minutes, des appels téléphoniques longue distance d&#39;affaires et la durée, en mois, d&#39;une batterie de voiture. . Il peut également être montré que la valeur du changement que vous avez dans votre poche ou votre sac à main suit approximativement une distribution exponentielle. . Les valeurs d&#39;une variable aléatoire exponentielle se produisent de la manière suivante. Il y a moins de grandes valeurs et plus de petites valeurs. . Par exemple, le montant d&#39;argent que les clients dépensent en un voyage au supermarché suit une distribution exponentielle. . Il y a plus de gens qui dépensent de petites sommes d&#39;argent et moins de gens qui dépensent de grandes sommes d&#39;argent. Les distributions exponentielles sont couramment utilisées dans les calculs de la fiabilité des produits ou de la durée de vie d&#39;un produit. . from scipy.stats import expon . x, scale = 100, 200 expon.pdf(x, loc=0, scale=1)# equivaut au paramètre lambda, tout comme en loi de Poisson . 3.720075976020836e-44 . La limite centrale . Si vous voulez comprendre la distribution du changement que les gens portent dans leurs poches, en utilisant le théorème de la limite centrale et en supposant que votre échantillon est suffisamment grand, vous constaterez que la distribution est normale et en forme de cloche. . Il existe deux formes alternatives du théorème, et les deux alternatives consistent à tirer des échantillons finis de taille $n$ à partir d&#39;une population avec une moyenne connue, $ mu$, et un écart type connu, $ sigma$. . La première alternative dit que si nous collectons des échantillons de taille $n$ avec un «assez grand $n$», calculons la moyenne de chaque échantillon et créons un histogramme de ces moyennes, alors l&#39;histogramme résultant aura tendance à avoir une forme de cloche normale approximative. . | La deuxième alternative dit que si nous collectons à nouveau des échantillons de taille $n$ qui sont &quot;assez grands&quot;, calculons la somme de chaque échantillon et créons un histogramme, alors l&#39;histogramme résultant aura à nouveau tendance à avoir une forme de cloche normale. . | L&#39;échantillon, $n$, qui est en d’autres termes «suffisamment grand», dépend de la population d&#39;origine à partir de laquelle les échantillons sont tirés (la taille de l&#39;échantillon doit être d&#39;au moins 30 ou les données doivent provenir d&#39;une distribution normale. . | Si la population d&#39;origine est loin d&#39;être normale, il faudra alors davantage d&#39;observations pour que les moyennes ou les sommes de l&#39;échantillon soient normales. L&#39;échantillonnage se fait avec remplacement. . | import numpy as np import matplotlib.pyplot as plt . Code issue de la page github de Rajesh Singh. . f = plt.figure(figsize=(18, 10)) def plotHist(nr, N, n_, mean, var0, x0): &#39;&#39;&#39; plots the RVs&#39;&#39;&#39; x = np.zeros((N)) sp = f.add_subplot(3, 2, n_ ) for i in range(N): for j in range(nr): x[i] += np.random.random() x[i] *= 1/nr plt.hist(x, 100, density=True, color=&#39;#348ABD&#39;, label=&quot; %d RVs&quot;%(nr)); plt.setp(sp.get_yticklabels(), visible=False) variance = var0/nr fac = 1/np.sqrt(2*np.pi*variance) dist = fac*np.exp(-(x0-mean)**2/(2*variance)) plt.plot(x0,dist,color=&#39;#A60628&#39;,linewidth=3,label=&#39;CLT&#39;,alpha=0.8) plt.xlabel(&#39;r&#39;) plt.xlim([0, 1]) leg = plt.legend(loc=&quot;upper left&quot;) leg.get_frame().set_alpha(0.1) N = 10000 # number of samples taken nr = ([1, 2, 4, 8, 16, 32]) mean, var0 = 0.5, 1.0/12 # mean and variance of uniform distribution in range 0, 1 x0 = np.linspace(0, 1, 128) for i in range(np.size(nr)): plotHist(nr[i], N, i+1, mean, var0, x0) plt.suptitle(&quot;Addition of uniform random variables (RVs) converge to a Gaussian distribution (CLT)&quot;,fontsize=20); . Le th&#233;or&#232;me central des limites pour les moyennes d&#39;&#233;chantillonnage . Supposons que X soit une variable aléatoire avec une distribution qui peut être connue ou inconnue (il peut s&#39;agir de n&#39;importe quelle distribution). En utilisant un indice qui correspond à la variable aléatoire, supposons: . a. $ mu_X =$ la moyenne de X . b. $ sigma_X =$ l&#39;écart type de X . Si vous tirez des échantillons aléatoires de taille, alors à mesure que $n$ augmente, la variable aléatoire $ bar{X}$ qui consiste en des moyennes d&#39;échantillon, tend à être normalement distribuée $ bar{X}_{ mapsto infty} approx N( mu_X, sigma_X)$ . Le théorème de limite central pour les moyennes d&#39;échantillon indique que si vous continuez à dessiner des échantillons de plus en plus grands (comme lancer un, deux, cinq et enfin dix dés) et à calculer leurs moyennes, les moyennes d&#39;échantillon forment leur propre distribution normale (la distribution d&#39;échantillonnage). . La distribution normale a la même moyenne que la distribution d&#39;origine et une variance égale à la variance d&#39;origine divisée par la taille de l&#39;échantillon. L&#39;écart type est la racine carrée de la variance, de sorte que l&#39;écart type de la distribution d&#39;échantillonnage est l&#39;écart type de la distribution d&#39;origine divisé par la racine carrée de $n$. . La variable $n$ est le nombre de valeurs moyennées ensemble, et non le nombre de fois où l&#39;expérience est effectuée. Pour le dire plus formellement, si vous tirez des échantillons aléatoires de taille acceptable, la distribution de la variable aléatoire $ bar{X}$, qui consiste en des moyennes d&#39;échantillon, est appelée distribution d&#39;échantillonnage de la moyenne. . La distribution d&#39;échantillonnage de la moyenne se rapproche de la distribution normale lorsque $n$, la taille de l&#39;échantillon, augmente. La variable aléatoire $ bar{X}$ a un score $z$ différent de celui de la variable aléatoire $X$. La moyenne $ bar{x}$ est la valeur de $ bar{X}$ dans un échantillon. $z = frac{ bar{x} − mu_x}{ frac{ sigma_x}{( sqrt[]{n})}}$ . $ mu_X$ est la moyenne de $X$ et de $ bar{X}$. . $ sigma_{ bar{x}} = frac{ sigma_x}{ sqrt[]{n}} = $ écart type de $ bar{X}$ et est appelée l’erreur srandard de la moyenne. . import numpy as np from scipy.stats import sem # Standard error of the means . a = np.arange(20).reshape(5,4) sem(a) . array([2.82842712, 2.82842712, 2.82842712, 2.82842712]) . sem(a, axis=None, ddof=0) . 1.2893796958227628 . Le th&#233;or&#232;me central de la limite des sommes . Supposons que X est une variable aléatoire avec une distribution qui peut être connue ou inconnue (cela peut être n&#39;importe quelle distribution) et supposons : . a. $ mu_X = $ la moyenne de $Χ$ . b. $ sigma_X = $ l&#39;écart type de X . Si vous tirez des échantillons aléatoires de taille acceptable, alors au fur et à mesure que vous l’augmenterez, la variable aléatoire $ Sigma{X}$ constituée de sommes a tendance à être normalement distribuée et $ Sigma{X} approx $$ N left( (n) ( mu_Χ), (n) ( sigma_Χ) right) $. . Le théorème central limite des sommes dit que si vous continuez à prélever des échantillons de plus en plus grands et à prendre leurs sommes, les sommes forment leur propre distribution normale (la distribution d&#39;échantillonnage), qui se rapproche d&#39;une distribution normale à mesure que la taille de l&#39;échantillon augmente. . La distribution normale a une moyenne égale à la moyenne originale multipliée par la taille de l&#39;échantillon et un écart type égal à l&#39;écart type d&#39;origine multiplié par la racine carrée de la taille de l&#39;échantillon. . La variable aléatoire $ Sigma{X}$ a le score-$z$ suivant associé à : . a. $ Sigma{x}$ est une somme . b. $z = frac{ Sigma{X} - (n)( mu_X)}{( sqrt[]{n})( sigma_x)}$ . i. $(n)( mu_X) = $ the mean of $ Sigma{X}$ . ii. $(n)( sigma_X)= $ standard deviation of $ Sigma{X}$ . Hypoth&#232;ses nulles et alternatives . Le test proprement dit commence par considérer deux hypothèses. On les appelle l&#39;hypothèse nulle et l&#39;hypothèse alternative. . Ces hypothèses contiennent des points de vue opposés. . H0, L&#39;hypothèse nulle : Il s&#39;agit d&#39;un énoncé de l&#39;absence de différence entre les moyennes ou proportions de l&#39;échantillon ou d&#39;absence de différence entre la moyenne ou la proportion d&#39;un échantillon et une moyenne ou une proportion de population. En d&#39;autres termes, la différence est égale à 0. . Ha, L&#39;hypothèse alternative : C&#39;est une affirmation sur la population qui est contradictoire avec H0 et ce que nous concluons lorsque nous rejetons H0. Puisque les hypothèses nulle et alternative sont contradictoires, vous devez examiner les preuves pour décider si vous avez suffisamment de preuves pour rejeter l&#39;hypothèse nulle ou non. . Les preuves se présentent sous la forme d&#39;échantillons de données. Une fois que vous avez déterminé l&#39;hypothèse prise en charge par l&#39;échantillon, vous prenez une décision. . Il existe deux options pour prendre une décision. Elles sont «rejeter H0» si les informations de l&#39;échantillon favorisent l&#39;hypothèse alternative ou «ne pas rejeter H0» ou «refuser de rejeter H0» si les informations de l&#39;échantillon sont insuffisantes pour rejeter l&#39;hypothèse nulle. Symboles mathématiques utilisés dans H0 et Ha : . H0 Ha . égal (=) | différent de (≠) ou supérieur à (&gt;) ou inférieur à (&lt;) | | . supérieur ou égal à (≥) | inférieur à (&lt;) | | . inférieur ou égal à (≤) | supérieur à (&gt;) | | . Sortie et erreurs de type I et II . Action Ho est vraiment Ho est vraiment . | Vrai | Fausse | | . Ne pas rejeter Ho | Sortie Correcte | Erreur Type II $ beta$ | | . Rejeter Ho | Erreur Type I $ alpha$ | Sortie Correcte | . $ alpha = $ probabilité de Type I error = P(Type I error)= probabilité de rejeter l’hypothèse nulle quand elle est vrai. . $ beta = $ probabilité de Type II error =P(Type II error)= probabilité de ne pas rejeter l’hypothèse nulle quand elle est fausse. . Deux moyennes de population avec des &#233;carts types inconnus . Les deux échantillons indépendants sont de simples échantillons aléatoires provenant de deux populations distinctes. . | Pour les deux populations distinctes : . | si la taille des échantillons est petite, les distributions sont importantes (devraient être normales) | si la taille des échantillons est grande, les distributions ne sont pas importantes (il n&#39;est pas nécessaire qu&#39;elles soient normales) | . Le test comparant deux moyennes de population indépendantes avec des écarts-types de population inconnus et éventuellement inégaux s&#39;appelle le test t d&#39;Aspin-Welch. La formule des degrés de liberté a été développée par Aspin-Welch. . La comparaison de deux moyennes de population est très courante. Une différence entre les deux échantillons dépend à la fois des moyennes et des écarts types. Des moyens très différents peuvent se produire par hasard s&#39;il existe une grande variation entre les échantillons individuels. Afin de tenir compte de la variation, nous prenons la différence des moyennes de l&#39;échantillon, $ bar{X_1} – bar{X_2}$, et divisons par l&#39;erreur standard afin de normaliser la différence. . Le résultat est une statistique de test t-score. Comme nous ne connaissons pas les écarts-types de la population, nous les estimons en utilisant les deux écarts-types de l&#39;échantillon de nos échantillons indépendants. Pour le test d&#39;hypothèse, nous calculons l&#39;écart type estimé, ou l&#39;erreur standard, de la différence des moyennes d&#39;échantillonnage, $ bar{X_1} – bar{X_2}$. . Pour des variables dépendantes : . from scipy.stats import norm from scipy.stats import ttest_rel . np.random.seed(12345678) # fix random seed to get same numbers rvs1 = norm.rvs(loc=5,scale=10,size=500) rvs2 = (norm.rvs(loc=5,scale=10,size=500) + norm.rvs(scale=0.2,size=500)) ttest_rel(rvs1,rvs2) . Ttest_relResult(statistic=0.24101764965300979, pvalue=0.8096404344581155) . rvs3 = (norm.rvs(loc=8,scale=10,size=500) + norm.rvs(scale=0.2,size=500)) ttest_rel(rvs1,rvs3) . Ttest_relResult(statistic=-3.9995108708727924, pvalue=7.308240219166128e-05) . Pour des variables indépendantes : . from scipy.stats import norm from scipy.stats import ttest_ind . rvs1 = norm.rvs(loc=5,scale=10,size=500) rvs2 = norm.rvs(loc=5,scale=10,size=500) ttest_ind(rvs1,rvs2) . Ttest_indResult(statistic=1.0763821159276765, pvalue=0.282016525733433) . ttest_ind(rvs1,rvs2, equal_var = False) . Ttest_indResult(statistic=1.0763821159276765, pvalue=0.2820166316269163) . ttest_ind underestimates p for unequal variances: . rvs3 = norm.rvs(loc=5, scale=20, size=500) ttest_ind(rvs1, rvs3) . Ttest_indResult(statistic=0.8102978439129243, pvalue=0.41796218423074705) . ttest_ind(rvs1, rvs3, equal_var = False) . Ttest_indResult(statistic=0.8102978439129243, pvalue=0.4180327843951014) . When n1 != n2, the equal variance t-statistic is no longer equal to the unequal variance t-statistic: . rvs4 = norm.rvs(loc=5, scale=20, size=100) ttest_ind(rvs1, rvs4) . Ttest_indResult(statistic=-0.19824285974122152, pvalue=0.8429224292670197) . ttest_ind(rvs1, rvs4, equal_var = False) . Ttest_indResult(statistic=-0.1321360117182417, pvalue=0.8951183654547593) . T-test with different means, variance, and n: . rvs5 = norm.rvs(loc=8, scale=20, size=100) ttest_ind(rvs1, rvs5) . Ttest_indResult(statistic=-0.7536436880813698, pvalue=0.45135982629995197) . ttest_ind(rvs1, rvs5, equal_var = False) . Ttest_indResult(statistic=-0.49564652531472553, pvalue=0.6211387007521327) . LA DISTRIBUTION CHI-SQUARE . La distribution du chi carré peut être utilisée pour trouver des relations entre deux choses, comme les prix d&#39;épicerie dans différents magasins. . Dans ce type de test d&#39;hypothèse, vous déterminez si les données «correspondent» ou non à une distribution particulière. Par exemple, vous pouvez soupçonner que vos données inconnues correspondent à une distribution binomiale. Vous utilisez un test du chi carré (ce qui signifie que la distribution de l&#39;hypothèse est le chi carré) pour déterminer s&#39;il existe un ajustement ou non. . L&#39;hypothèse nulle et les hypothèses alternatives pour ce test peuvent être écrites dans des phrases ou peuvent être énoncées sous forme d&#39;équations ou d&#39;inégalités. La statistique de test pour un test de qualité d&#39;ajustement est : $ Sigma_{k} frac{(O − E)^2}{E}$ . où: . O = valeurs observées (données) | E = valeurs attendues (de la théorie) | k = le nombre de cellules de données différentes ou catégories | . La probabilité de masse est : $f(x, k) = frac{1}{2^{ frac{k}{2}} Gamma( frac{k}{2})}x^{ frac{k}{2-1}}exp( frac{-x}{2})$ . pour $x gt{0}$ et $k gt{0}$ (degrés de liberté, noté df dans l’implémentation . from scipy.stats import chi2 . x, df, var, scale = 1, 55, 0, 40 chi2.cdf(x, df, loc, scale) . 7.96460240704433e-82 . Effectuer et interpr&#233;ter le test du chi carr&#233; des tests d&#39;hypoth&#232;se d&#39;ind&#233;pendance. . Les tests d&#39;indépendance impliquent l&#39;utilisation d&#39;une table de contingence des valeurs (données) observées. La statistique de test pour un test d&#39;indépendance est similaire à celle d&#39;un test de qualité d&#39;ajustement : $ Sigma_{(i cdot j)} frac{(O – E)^2}{E}$ . où: . O = valeurs observées | E = valeurs attendues | i = le nombre de lignes dans le tableau | j = le nombre de colonnes du tableau | . Il existe $i cdot j$ termes de la forme $ frac{(O – E)^2}{E}$. . Un test d&#39;indépendance détermine si deux facteurs sont indépendants ou non. . Statistique de test : Utiliser une statistique de test $ chi^2$. . Il est calculé de la même manière que le test d&#39;indépendance. Degrés de liberté (df) $df = $ nombre de colonnes $- 1$ . Conditions requises : Toutes les valeurs du tableau doivent être supérieures ou égales à cinq. . Utilisations courantes : Comparaison de deux populations. . Par exemple: hommes vs femmes, avant vs après, est vs ouest. La variable est catégorique avec plus de deux valeurs de réponse possibles. . Effectuer et interpr&#233;ter des tests d&#39;hypoth&#232;se d&#39;homog&#233;n&#233;it&#233; du chi carr&#233;. . Le test de qualité d&#39;ajustement peut être utilisé pour décider si une population correspond à une distribution donnée, mais il ne suffira pas de décider si deux populations suivent la même distribution inconnue. Un test différent, appelé test d&#39;homogénéité, peut être utilisé pour tirer une conclusion quant à savoir si deux populations ont la même distribution. Pour calculer la statistique de test pour un test d&#39;homogénéité, suivez la même procédure que pour le test d&#39;indépendance. La valeur attendue de chaque cellule doit être d&#39;au moins cinq pour que vous puissiez utiliser ce test. . Hypothèses H0: Les distributions des deux populations sont les mêmes. . Ha: Les distributions des deux populations ne sont pas les mêmes. . from scipy.stats import chi2_contingency . obs = np.array([[10, 10, 20], [20, 20, 20]]) chi2_contingency(obs) # return [chi, pvalue, df, ndarray] . (2.7777777777777777, 0.24935220877729622, 2, array([[12., 12., 16.], [18., 18., 24.]])) . R&#233;aliser et interpr&#233;ter des tests d&#8217;hypoth&#232;se de variance unique du chi carr&#233; . Au plus d&#39;une seule variance suppose que la distribution sous-jacente est normale. Les hypothèses nulles et alternatives sont énoncées en termes de variance de la population (ou écart type de la population). La statistique du test est: . $ frac{(n- 1)s^2}{σ^2}$ . où : . $n = $ le nombre total de données | $s^2 = $ variance de l&#39;échantillon | $ sigma^2 = $ variance de la population Vous pouvez penser à la variable aléatoire dans ce test. Le nombre de degrés de liberté est df = n- 1. Un test d&#39;une seule variance peut être à droite, à gauche ou bilatéral. L&#39;exemple 11.10 vous montrera comment configurer les hypothèses nulles et alternatives. Les hypothèses nulle et alternative contiennent des déclarations sur la variance de la population. | . from scipy.stats import fligner . a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99] b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05] c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98] stat, p = fligner(a, b, c) p . 0.00450826080004775 .",
            "url": "https://blog.rboyrie.info/statistics/",
            "relUrl": "/statistics/",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Quelques Observations Quotidiennes",
            "content": "2021-02-22 . La vision de Copenhague, qui situe le monde quantique dans un autre monde que le monde classique, serait parfait si nous ne voulions pas voir du monde quantique autre chose qu’une vision théorique et mathématique. C’est un monde où les particules se comportent en théorie comme une onde, car rien n’interfère avec leur état. Elles peuvent ainsi être en état de superposition mais sans que nous puissions procéder à une mesure d’une interférence. . Toutefois si nous considérons que la particule est en même temps de nature ondulatoire et corpusculaire, nous obtenons dans ce cas, une vision du monde quantique telle que la décrivent Louis de Broglie et David Bohm et qui s’oppose à la vision de Copenhague. Pour faire simple, l’onde est perçue comme une trame sur laquelle se déplace le corps, un peu comme un surfeur. Mais pour que l’onde agisse sur la particule en permanence et de manière instantanée, quelque-soit la distance entre elles, la transmission devrait se produire à une vitesse supérieure à celle de la lumière, ce qui n’est pas concevable depuis la théorie de la relativité d’Einstein. . Enfin, il peut-être plus intéressant encore de diviser l’univers étudié en autant d’états possibles que peuvent prendre ses particules. En dépliant cette arborescence, nous nous rendons compte qu’il s’agit de duplication de l’univers, un pour chaque état des particules. Le nombre de branches pourra donner le vertige quand les états des particules ne seront pas simplement binaires. . 2021-02-06 . Le grand dilemme de cet avenir vers 2030, de ce futur que nous imaginons infructueux quand nous sommes pessimistes, est dans ce qui représente la prochaine révolution, la physique quantique. Imaginons un seul instant que l’ensemble des positions des électrons dans notre tête soient mesurées par une puce dans notre cerveau relié à un ordinateur quantique par le réseau des réseaux, sans doute dès lors, sera-t-il possible, pour un programme quantique, de deviner nos pensées des minutes qui suivent la mesure. La notion de libre arbitre risque de devenir flou. La fonction d’onde de Schrödinger montre déjà comment se comporte un électron dans son nuage électronique : . $ih frac{ delta psi}{ delta t} = - frac{h^2}{2m} nabla^2 psi+V psi$ . h est une constante, $ psi$ est la fonction d’onde (probabilité de trouvé un électron dans un nuage), V est la description des forces que subit l’électron, et le reste (i, $ frac{ delta}{ delta t}$, $ nabla$ dit nabla) sont des outils mathématiques. Les prédictions seront sans doute plus facile grâce aux ordinateurs quantiques, mais à quel prix pour la liberté de penser ? Les gens seront-ils rangés par paquets bien homogènes ? . 2021-02-02 . Hier je vous parlais d’une idée qui m’avait semblé intéressante, pourquoi, parce que c’est dans l’air du temps et dans mon tempérement également. Mais je sens que je ne suis pas allé assez loin pour atteindre le but que je m’étais fixé qui était celui de dégager mes impressions sur le sujet, donc j’ai trouvé heureusement, ce matin sagement, une énième barrière technologique. Bien que cela bride un peu l’imaginaire, je vais quand même l’évoquer en entrant immédiatement dans le vif du sujet. . C’est tout simplement la perte d’information entre les signaux digitaux et analogiques. La clé de ce sujet est la compression de l’information contenue dans le signal. Il aurait pu sembler regrettable pour un certain ingénieur du son de ma connaissance d’écouter une musique sur un mp3, et pour lui, de plus, il aurait toujours mieux valu un format moins compressé tel que le flac (bien que d’autres formats existent). Mais la vérité c’est que le format mp3 satisfait la majeur parti du public. . Sans doute parce que l’appréciation de la musique ainsi que son interprétation et son réglage n’ont pas la même valeur selon qu’une personne écoute la musique par le ventre ou bien comme une collection de paramètres. L’enjeu de la qualité et de la quantité peut faire débat longtemps, je crois, malheureusement. Qu’en est-il des compositeurs qui entendent les instruments dans leur tête ? Une puce pourrait-elle remplacer cette fonction judicieusement dans leur cerveau ? . 2021-02-01 . La journée commence toujours à la fin lorsque je dois reprendre mes notes pour vous écrire un compte-rendu. Cette fois, j’ai pensé aux puces intégrées dans le cerveau, celles-là mêmes qui pourraient transformer tout novice de la finance en Warren Buffet par une seule opération, certe un peu invasive, mais vous n’aurez rien sans rien. . À quoi bon rêver de ces puces, et surtout, pourrons-nous réellement penser de la même manière une fois qu’elles nous seront implantées ? Une puce ayanc une certaine fréquence d’horloge, alors que le flux de la pensée est irrégulier. Devrons-nous par conséquent apprendre à penser en écoutant un métronome, sans que ça soit un danger majeur pour notre comportement humain. . Les risques seront-ils minimes ou élevés, et pourrons-nous continuer toutes nos activités habituelles ? Tandis que les puces pourraient permettre aux anciens de devenir aussi malin que les petits nouveaux ou en sens contraire, en tout cas, la rotation de la Terre ne s’arrêtera pas de tourner pour si peu… . 2021-01-31 . Encore un autre jour de la période Covid-19 passée dans mon château-fort. En ayant passé des heures à chercher la cause de mon problème pour trouver le remède qui n’était rien de plus que de faire sans faire. Aimer l’eau, l’air, haïr la haine, voilà un résumé de ce petit manuel du terrien. . 2021-01-30 . Les aiguilles ont fait un tour de cadran et je goutte une nouvelle liberté d’action pour organiser un nouveau projet professionnel, après la restructuration de mon service pro, j’entre dans une période de recherche d’une nouvelle entreprise pour un poste à l’identique. Je suis confiant car je ne suis pas le seul à dire que la Data Science et l’IA sont des domaines qui ont un avenir, mais, je l’espère pour moi, aussi en France ! Car malgré les autres grandes puissances et la diversification du hardware qui ira en augmentant (tel que nous pouvons l’imaginer), nous avons des Grands Groupes, et une French Tech avec ses startups ou même ses licornes. . 2021-01-29 . Demain j’aurai sans doute terminé mon travail sur les outils statistiques que je m’étais fixé le mois dernier. Connaître bientôt la satisfaction du travail accompli me permet de garder espoir dans le futur tout en pensant à autre chose qu’au stress hydrique que connaissent déjà plusieurs villes sur Terre. .",
            "url": "https://blog.rboyrie.info/vie%20priv%C3%A9e/2021/01/29/Daily-Observations.html",
            "relUrl": "/vie%20priv%C3%A9e/2021/01/29/Daily-Observations.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Signification statistique",
            "content": "Humilit&#233; . Difficile de parler de vérité absolue dans un résultat statistique, le scientifique reconnaît presque toujours que son résultat d’analyse dépend de la quantité et de la qualité de ses data. Certains pourront y voir un problème de culture scientifique, entre le statisticien, les big data… gageons que l’adage &quot;Vérité en deçà des Pyrénées, mensonges au-delà&quot; puisse illustrer à merveille les représentations de l’absence d’une vérité unique et immuable, ne serait-ce que parce que la sagesse nous enseigne que tout est éphémère dans l’univers. La chance et l’aléatoire prennent beaucoup de place dans les statistiques et valider une hypothèse nécessite la réduction maximale de leur impact. . Observation . De la fabrication d’une hypothèse à sa validation, tout scientifique se passionnera en se représentant les effets et les causes d’un événement. Les mathématiques statistiques sont un moyen de démontrer qu’entre l’hypothèse sans cause, et l’hypothèse avec cause, il y a une signification statistique, en-deçà de laquelle il est possible de conclure de la valeur de l’effet sur la cause. Mais ce n’est pas la panacé, et bien des erreurs se produisent dans la pratique (un exemple est l’effet placebo) et malgré leur origine théorique, la signification statistique est juste ce que l’on fait de plus scientifique actuellement. Si l’on se penche sur les expériences autrement plus difficile à produire en physique quantique, ou les conditions des expériences influent énormément sur les résultats, cela entraîne que la reproductibilité est parfois impossible et c’est très ennuyeux. Comment alors confirmer les résultats d’un chercheur. Toutefois, on arrête pas la marche du progrès ainsi, et rester les bras croisés et ne croire aucun rapport scientifique demeure de l’ordre de l’obscurantisme, et rien ne saurait remplacer le pragmatisme de la science, ne serait-ce que pour les vaccins. De plus, la théorie statistique permet aussi de prédire, parfois en extrapolant avec risque, néanmoins, les observations empiriques permettent bien sûr de penser a posteriori, le seul hic, c’est que pour valider une hypothèse, il est nécessaire d’essayer d’observer l’expérience un grand nombre de fois. . Multiplier les essais . Pour démontrer que les lois statistiques peuvent décrire la vérité, il est nécessaire de reproduire les conditions de l’expérience un grand nombre de fois. Dans l’expérience du lancé de 2 dés en simultané, par exemple, nous savons qu’il ne peut pas y avoir de corrélation entre le résultat des deux dés, à moins d’un facteur de chance ou d’un aléas, et celui-ci diminue plus le nombre de lancés augmente. Pour démontrer que plus le nombre d’essais est grand et plus le facteur chance et d’aléas disparaissent, nous allons représenter la distribution des corrélations des résultats des lancés. Nous savons que si le nombre d’essais est infini, la corrélation tend vers 0 par bon sens, mais nous allons créer la densité de probabilité en quelques lignes de programme Python, et nous découvrirons cette forme de cloche si particulière de Gauss. . !pip install numpy . Chargement des modules . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns . Nous créons la fonction d’expériences pour créer la distribution . def dices(N=10, k=500): &quot;&quot;&quot;Renvoie une distribution corrélation entre N lancés de deux dés pour k réalisations d’expérience. &quot;&quot;&quot; np.random.seed(42) # 42 comme graine d’aléas cf. œuvre de Douglas Adams qui l’a popularisé comme # la réponse à l’ultime question de la vie, de l’Univers et de toute chose. N = N k = k distrib = [] for i in range(k): XY = np.random.randint(1,6,(2,N)) rho = round(np.corrcoef(XY)[0,1],2) distrib.append(rho) return distrib . plt.figure(figsize=(6,3), dpi= 80) # Dimensionnement du tableau kwargs = dict(hist_kws={&#39;alpha&#39;:.6}, kde_kws={&#39;linewidth&#39;:2})# Dictionnaire de gestion colorimétrique, épaisseur sns.distplot(dices(10,5), color=&quot;dodgerblue&quot;, label=&quot;Compact&quot;, **kwargs) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x268ec11b040&gt; . Ici, visuellement, il est difficile d’être en mesure de faire de l’inférence statistique car la forme de cloche, que l’on approche très souvent à la loi Normale, n’est pas visible. Nous allons refaire l’expérience en augmentant le nombre d’expériences à 2000 fois. . plt.figure(figsize=(6,3), dpi= 80) # Dimensionnement du tableau kwargs = dict(hist_kws={&#39;alpha&#39;:.6}, kde_kws={&#39;linewidth&#39;:2})# Dictionnaire de gestion colorimétrique, épaisseur sns.distplot(dices(10, 2000), color=&quot;dodgerblue&quot;, label=&quot;Compact&quot;, **kwargs) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x268ec127700&gt; . En conclusion, nous apercevons que nous retrouvons une courbe en forme de cloche dont le pic indique que la corrélation n’existe pas entre les 10 lancés de deux dés. À partir de cette courbe de Gauss, nous appuyant sur la loi Normale, nous allons pouvoir exprimer un intervalle de confiance à l’intérieur duquel, nous pourrons circonscrire tout résultat des expériences reproduisant nos conditions expérimentales. Grâce à cette certitude, le scientifique peut détecter les cas qui sortent de l’ordinaire, qui déconstruisent une hypothèse en favorisant une autre, par le biais de la signification statistique (notée $ alpha$ par convention et souvent égale à 5%). .",
            "url": "https://blog.rboyrie.info/math/statistiques/2020/12/16/Signification-statistique.html",
            "relUrl": "/math/statistiques/2020/12/16/Signification-statistique.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Matrices",
            "content": "Domaine . Les matrices sont des objets mathématiques que l’on regroupe dans l’Algèbre Linéaire. Elles permettent d’exprimer des vecteurs dans des espaces à n dimensions. Un vecteur peut être une position, votre géolocalisation sur la Terre, ou tout simplement une information telle que votre rythme cardiaque ou votre revenu. C’est pour cela que notre monde à 3 dimensions spatiales et extensible peuvant recevoir autant de dimension que nécessaire pour décrire la position d’une position appelé vecteur dans un espace à n dimension. . Op&#233;rateur . Quels sont les opérateurs des matrices : . Le produit scalaire : équivalent d’une projection d’un premier vecteur, sur le second. | Le produit vectoriel : c’est la recherche du vecteur perpendiculaire à un plan formé par deux vecteurs, dont la norme est aussi l’aire de ce plan. | La norme : la racine de la somme des carrés des longueurs d’un vecteur sur chaque direction de l’espace à n dimension. | Le déterminant : c’est une caractéristique quantitative des normes de vecteurs dans un espace à n dimensions. Si n vaut 2 ou 3, c’est l’aire ou le volume. | Le vecteur unitaire : c’est un vecteur colinéaire et de même direction qu’un autre, mais sa norme vaut 1. | Le valeur propre : coefficient d’étirement d’un vecteur propre, d’une application linéaire dans un espace vectoriel. | La vecteur propre : vecteur qui est juste étiré ou raccourci lors d’une transformation linéaire dans un espace vectoriel. | Utilisation de Python pour chaque op&#233;rateur . Pour réaliser nos calculs nous aurons besoin de la bibliothèque numpy afin de construire nos tableaux de valeurs. Pour l’installation dans Jupyter-notebook, nous pouvons utiliser la commande suivante : . !pip install scipy numpy sympy . Chargement des biblioth&#232;ques . import numpy as np from numpy import linalg as LA from numpy.linalg import eig from sympy import * . Produit scalaire . ### Produit scalaire: produit de deux vecteurs f = np.array([1,2,4,6,7]) g = np.array([4,5,3,8,3]) ### 1*4+2*5+4*3+6*8+7*3 np.dot(f, g) . 95 . Dans l’étude du produit vectoriel et de la norme ci-dessous, nous considérons semblables les représentations géométriques et algébriques de vecteurs. Cela nous permet de dire que la norme d’un vecteur h par le produit vectoriel de deux vecteurs f, g est égale à l’aire du plan formé par ces deux derniers vecteurs. . Produit vectoriel . ### Produit vectoriel équivaut au produit extérieur: f x g = f ∧ g f = np.array([3,5,2]) g = np.array([5,7,4]) ### i(5*4-7*2)+j(5*2-3*4)+k(3*7-5*5) np.cross(f, g) . array([ 6, -2, -4]) . Norme . ### Produit vectoriel équivaut au produit extérieur: f x g = f ∧ g soit le vecteur h f = np.array([3,5,2]) g = np.array([5,7,4]) ### i(5*4-7*2)+j(5*2-3*4)+k(3*7-5*5) h = np.cross(f, g) ### La norme de h vaut le produit extérieur f ∧ g LA.norm(h,1) . 12.0 . Le d&#233;terminant . a = np.array(([1,2],[-3,4])) int(np.linalg.det(a)) . 10 . Nous pouvons tout à fait utiliser plutôt le module sympy et faire le calcul comme suivant. . M = Matrix([[1,2],[-3,4]]) M . $ displaystyle left[ begin{matrix}1 &amp; 2 -3 &amp; 4 end{matrix} right]$ M.det() . $ displaystyle 10$ Le vecteur unitaire . Le vecteur unitaire a une magnitude de 1, sa formule est $ hat{v} = frac{1}{ lvert lvert v rvert rvert} cdot v$ . . v = np.arange(3) v_hat = v / np.linalg.norm(v) v_hat . array([0. , 0.4472136 , 0.89442719]) . La valeur propre et le vecteur propre . Dans une application linéaire, un vecteur propre est la transformation linéaire telle celle de la multiplication d’un vecteur par une valeur propre (scalaire). . $T(v)= lambda v$ . En somme, pour une transformation donnée, c’est les vecteurs qui sont sur la ou les ligne(s) de transformation(s) et pour qui tout vecteur se trouvant dessus sera soit étiré soit réduit mais dont la direction ne changera pas. . a = np.array([[2, 0, 0], [0, 3, 4], [0, 4, 9]]) values , vectors = eig(a) print(values) print(vectors) . [11. 1. 2.] [[ 0. 0. 1. ] [ 0.4472136 0.89442719 0. ] [ 0.89442719 -0.4472136 0. ]] . De même, avec le module python sympy, nous pouvons déclarer une matrice M et exécuter les calculs. . M = Matrix([[2, 0, 0], [0, 3, 4], [0, 4, 9]]) M . $ displaystyle left[ begin{matrix}2 &amp; 0 &amp; 0 0 &amp; 3 &amp; 4 0 &amp; 4 &amp; 9 end{matrix} right]$ M.eigenvals() . {11: 1, 2: 1, 1: 1} . M.eigenvects() . [(1, 1, [Matrix([ [ 0], [-2], [ 1]])]), (2, 1, [Matrix([ [1], [0], [0]])]), (11, 1, [Matrix([ [ 0], [1/2], [ 1]])])] . M = Matrix([[2, 4], [3, 13]]) M . $ displaystyle left[ begin{matrix}2 &amp; 4 3 &amp; 13 end{matrix} right]$ M.eigenvals() . {14: 1, 1: 1} .",
            "url": "https://blog.rboyrie.info/math/matrices/2020/12/10/Matrix.html",
            "relUrl": "/math/matrices/2020/12/10/Matrix.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "One-way ANOVA",
            "content": "O&#249; s&#8217;utilise cette m&#233;thode . Nous trouvons cette méthode d’analyse de la variance dans les sciences comme la psychologie, les sciences sociales, les sciences naturelles. Elle permet l’analyse d’une variable indépendante dans plusieurs groupes : un sociologue pourra mesurer le niveau de salaire d’une personne en fonction de son niveau d’étude, un consommateur pourra comparer la consommation moyenne de véhicules suivant le modèle… Pour mesurer plusieurs groupes, les statisticiens ont développé la méthode d’analyse de la variance ou ANOVA. La plus simple, qui analyse une seule variable indépendante est la one-way ANOVA. . Principe . Le but de la méthode one-way ANOVA est de trouver s’il existe une différence statistiquement significative dans les moyennes de plusieurs groupes. La méthode utilise la variance pour vérifier que les moyennes sont semblables. . Hypoth&#232;ses de travail . Chaque échantillon est pris d’une population dont la distribution est considérée comme suivant une loi Normale. | L’échantillonnage est aléatoire, et le tirage de chaque groupe est indépendent. | Les populations des groupes ont un écart type équivalent ou égal. | Le facteur est une variable de catégorie | La réponse est une variable numérique | Test d&#8217;hypoth&#232;se . L’hypothèse nulle est le cas dans lequel toutes les moyennes sont égales. L’hypothèse alternative est qu’au moins un couple de moyennes n’est pas équivalent. . Calcul du test d&#8217;hypoth&#232;se . Pour réaliser le calcul, nous utilisons la distribution F. Cette statistique est un ratio (un rapport, une fraction) comprenant deux degrés de liberté, un pour le numérateur, l’autre pour le dénominateur. . Note :La distribution F dérive du t-test et de la distribution Student, qui est simplement l’élévation au carré de celle-là. Il est préférable d’utiliser la méthode one-way ANOVA quand on veut étudier plusieurs groupes au-lieu que de réaliser de multiple t-test à cause du risque $ alpha$ qui est le rejet de l’hypothèse nulle alors qu’elle est vraie. . Le calcul de Ratio-F . $F = frac{MS_{between}}{MS_{within}}$ La valeur de F est le rapport entre la moyenne de la variance au carré entre les groupes, et la moyenne de la variance au carré dans les groupes (dû à l’échantillonage). . $MS_{between} = frac{SS_{between}}{df_{between}} = frac{SS_{between}}{k - 1}$ avec $k$ comme nombre de groupe . $MS_{within} = frac{SS_{within}}{df_{within}} = frac{SS_{within}}{n-k}$ avec $n$ comme somme de la quantité d’observations pour tous les groupes. . Quand les groupes ont la même taille, la valeur de F revient à : . $F= frac{n cdot {s_{ bar{x}}}^{2}}{s^{2}}$ . Nous détaillerons plus le calcul dans l’exemple suivant. . Importation des modules scientifiques . Pour réaliser nos calculs nous aurons besoin de la bibliothèque scientifique de python scipy et également de numpy pour construire nos tableaux de valeurs. Pour l’installation dans Jupyter-notebook, nous pouvons utiliser la commande suivante : . !pip install scipy numpy . Requirement already satisfied: scipy in c: programdata anaconda3 lib site-packages (1.5.0) Requirement already satisfied: numpy in c: programdata anaconda3 lib site-packages (1.18.5) . Nous importons la méthode pour la méthode one-way ANOVA, la fonction de densité de F et le module numpy . from scipy.stats import f_oneway from scipy.special import fdtrc import numpy as np . Exemple . Nous collectons des données du supermarché du coin afin de réaliser une analyse de la variance sur trois produits : des fruits, des légumes et des pains. Nous collectons nos données de manière aléatoire à partir des rayons de notre supermarché en prenant le prix au kilos de huit produits non contigus dans chacun des rayons. Nous retrouvons nos 3 catégories et les valeurs numériques (prix par kilogramme) comme suivant : . fruits = np.array([1.99, 3.05, 2.60, 1.45, 2.99, 2.91, 2.72, 3.15]) legumes = np.array([1.25, 1.75, 1.40, 1.66, 2.20, 2.70, 3.00, 1.99]) pains = np.array([0.90, 1.25, 1.10, 1.55, 2.20, 3.00, 3.05, 1.05]) . L’hypothèse nulle $H_{0}$ est que les moyennes des prix de ces trois catégories de produit ne sont pas statistiquement différentes de manière significative au vue de notre jeu de données. Nous pourrons prendre une pvalue de 5%, ce qui représente une certitude de 95% quant à notre résultat, en statistique. . Pour les fruits, les légumes, et les pains, nous avons les valeurs statistiques suivantes : . fruits_mean = fruits.mean() fruits_var = fruits.var() fruits_cnt = len(fruits) fruits_stat = &#39;La moyenne des fruits est &#39; + f&#39;{fruits_mean:0.3}&#39; + &#39;, La variance est &#39; + f&#39;{fruits_var:0.2}&#39; + &#39;, Le nombre d’observations est &#39; + f&#39;{fruits_cnt}&#39; fruits_stat . &#39;La moyenne des fruits est 2.61, La variance est 0.31, Le nombre d’observations est 8&#39; . legumes_mean = legumes.mean() legumes_var = legumes.var() legumes_cnt = len(legumes) legumes_stat = &#39;La moyenne des légumes est &#39; + f&#39;{legumes_mean:0.3}&#39; + &#39;, La variance est &#39; + f&#39;{legumes_var:0.2}&#39; + &#39;, Le nombre d’observations est &#39; + str(legumes_cnt) legumes_stat . &#39;La moyenne des légumes est 1.99, La variance est 0.33, Le nombre d’observations est 8&#39; . pains_mean = pains.mean() pains_var = pains.var() pains_cnt = len(pains) pains_stat = &#39;La moyenne des pains est &#39; + f&#39;{pains_mean:0.3}&#39; + &#39;, La variance est &#39; + f&#39;{pains_var:0.2}&#39; + &#39;, Le nombre d’observations est &#39; + str(pains_cnt) pains_stat . &#39;La moyenne des pains est 1.76, La variance est 0.67, Le nombre d’observations est 8&#39; . Estimation du ratio-F, par la distribution Fischer . Pour parvenir à notre résultat, nous ferons plusieurs calculs. . Les degrés de libertés sont (pour n = 24 , k = 3) : . $df_{num} = 3 - 1$ soit 2 . et . $df_{denom} = 24 - 3$ soit 21 . La valeur de F que nous cherchons est $F_{2,21}$ soit dans la table des valeurs de la distribution F : 3.46 . alldata = np.concatenate([[fruits,legumes,pains]]) k = len(alldata) # Nombre de catégories ...2 N = alldata.shape[0] * alldata.shape[1] # Nombre d’observations ...24 n = alldata.shape[1] # Nombre d’observations dans chaque catégorie ...8 . df_num = k - 1 df_denom = N - k . $SS_{between} = frac{ sum{x^2}}{n} - frac{( sum{x})^2}{N}$ . SS_between = (np.array([fruits.sum()**2, legumes.sum()**2, pains.sum()**2]).sum()/n ) - (np.array([fruits.sum(), legumes.sum(), pains.sum()]).sum()**2 /N) . $SS_{tot} = sum{x^2} - frac{( sum{x})^2}{N}$ . SS_tot = (np.array([fruits**2, legumes**2, pains**2]).sum())- (np.array([fruits, legumes, pains]).sum()**2 /24) . $SS_{within} = SS_{tot}- SS_{between}$ . SS_within = SS_tot - SS_between . MS_between = SS_between / df_num . MS_within = (SS_within) / df_denom . F = MS_between / MS_within F . 3.0596582667896874 . $pvalue = P(F&gt; 3.46) = 0.0682$ . pvalue = fdtrc(df_num, df_denom, F) pvalue . 0.06821436223117332 . En conclusion, d’après nos données, nous sommes sûr à 95% que l’hypothèse nulle $H_{0}$ doit-être rejetée. . La même conclusion peut être calculée, plus rapidement, avec la méthode f_oneway() du module scipy. . f_oneway(fruits, legumes, pains) . F_onewayResult(statistic=3.059658266789707, pvalue=0.06821436223117218) . Restituer graphiquement le r&#233;sultat . Pour observer nos résultats et comprendre visuellement le résultat statistique qui indique que les moyennes sont différentes, nous allons importer deux autres modules en Python pour dessiner nos données. . import pandas as pd import matplotlib.pyplot as plt . df = pd.DataFrame(alldata.T, columns=[&#39;Fruits&#39;,&#39;Legumes&#39;,&#39;Pains&#39;]) # Dessiner un graphique de boîte à moustache boxplot = df.boxplot( figsize=(12, 8)) # Sauvegarder vos boîtes à moustaches plt.savefig(&#39;image.png&#39;) . Grâce à ce graphique qui représente en plus, très bien les quartiles, nous pouvons voir que les fruits contiennent une valeur aberrante, un outsider, très loin du reste du paquet de l’échantillon. .",
            "url": "https://blog.rboyrie.info/statistiques/anova/2020/11/12/One-way-ANOVA.html",
            "relUrl": "/statistiques/anova/2020/11/12/One-way-ANOVA.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://blog.rboyrie.info/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.rboyrie.info/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.rboyrie.info/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}